import google.genai as genai
from google.genai import types
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
import logging
import os
import time
import asyncio
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

if logger.handlers:
    for handler in logger.handlers:
        logger.removeHandler(handler)

# ë¡œê¹… ë ˆë²¨ ì„¤ì • (ë” ìƒì„¸í•œ ë¡œê·¸ë¥¼ ìœ„í•´ INFOë¡œ ì„¤ì •)
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)


# LangChain imports for pandas agent (with fallback handling)
LANGCHAIN_AVAILABLE = False
ChatGoogleGenerativeAI = None
ChatOpenAI = None
create_pandas_dataframe_agent = None
ZERO_SHOT_REACT_DESCRIPTION = "zero-shot-react-description"
TOOL_CALLING_DESCRIPTION = "tool-calling"

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain_experimental.agents import create_pandas_dataframe_agent
    from langchain_openai import ChatOpenAI

    # ìµœì‹  LangChain ë²„ì „ì—ì„œ agent_types ê²½ë¡œ ë³€ê²½
    try:
        from langchain_classic.agents.agent_types import AgentType as LangChainAgentType

        ZERO_SHOT_REACT_DESCRIPTION = LangChainAgentType.ZERO_SHOT_REACT_DESCRIPTION
    except ImportError:
        # fallback: ì§ì ‘ ë¬¸ìì—´ ì •ì˜ (ìµœì‹  ë²„ì „ì—ì„œëŠ” ë¬¸ìì—´ë„ ì§€ì›)
        ZERO_SHOT_REACT_DESCRIPTION = "zero-shot-react-description"

    LANGCHAIN_AVAILABLE = True
    logger.info("LangChain pandas agent imports ì„±ê³µ")
except ImportError as e:
    logger.warning(f"LangChain imports ì‹¤íŒ¨: {e}")
    LANGCHAIN_AVAILABLE = False

    # Fallback dummy classes
    class DummyChatGoogleGenerativeAI:
        def __init__(self, *args, **kwargs):
            raise ImportError("LangChain packages not available")

    class DummyChatOpenAI:
        def __init__(self, *args, **kwargs):
            raise ImportError("LangChain packages not available")

    def dummy_create_pandas_dataframe_agent(*args, **kwargs):
        raise ImportError("LangChain packages not available")

    ChatGoogleGenerativeAI = DummyChatGoogleGenerativeAI
    ChatOpenAI = DummyChatOpenAI
    create_pandas_dataframe_agent = dummy_create_pandas_dataframe_agent


class HybridRAGSystem:
    def __init__(self, llm_provider: str = "openai"):
        self.llm_provider = llm_provider.lower()
        self.client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
        self.llm = self.client.models.generate_content
        self.qa_chain = None
        self._pandas_llm = None
        # í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ íŒë‹¤ìŠ¤ ë¶„ì„ ë‹¨ê³„ ì„¤ì •
        self.pandas_analysis_stages = int(os.getenv("PANDAS_ANALYSIS_STAGES", "2"))
        logger.debug(
            f"HybridRAGSystem initialized with llm_provider={self.llm_provider}, pandas_analysis_stages={os.getenv('PANDAS_ANALYSIS_STAGES')}"
        )

    def _get_pandas_llm(self):
        """LangChain pandas agentë¥¼ ìœ„í•œ LLM ì´ˆê¸°í™” (lazy loading)"""
        if not LANGCHAIN_AVAILABLE:
            logger.error("LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ - pandas agent ìƒì„± ë¶ˆê°€")
            raise ImportError("LangChain íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        if self._pandas_llm is None:
            try:
                if self.llm_provider == "gemini":
                    if ChatGoogleGenerativeAI is None:
                        raise ImportError("ChatGoogleGenerativeAI not available")

                    model = "gemini-3-flash-preview"

                    self._pandas_llm = ChatGoogleGenerativeAI(
                        model=model,
                        temperature=0,
                        google_api_key=os.getenv("GOOGLE_API_KEY"),
                        # convert_system_message_to_human=True,
                        generate_content_config=types.GenerateContentConfig(
                            thinking_config=types.ThinkingConfig(
                                thinking_level="LOW",
                            )
                        ),
                    )
                    logger.info(
                        f"LangChain ChatGoogleGenerativeAI ì´ˆê¸°í™” ì„±ê³µ - ì‚¬ìš© model: {model}"
                    )
                elif self.llm_provider == "openai":
                    model = "qwen/qwen3-235b-a22b-2507"
                    self._pandas_llm = ChatOpenAI(
                        base_url="https://openrouter.ai/api/v1",
                        # base_url="https://api.groq.com/openai/v1",
                        model=model,
                        temperature=0,
                        openai_api_key=os.getenv("OPENAI_API_KEY"),
                        default_headers={
                            "HTTP-Referer": "http://localhost:8501",
                            "X-Title": "MyManger Frontdesk",
                        },
                    )
                    logger.info(
                        f"LangChain ChatOpenAI ì´ˆê¸°í™” ì„±ê³µ - ì‚¬ìš© model: {model}"
                    )
                else:
                    raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” LLM ì œê³µì—…ì²´: {self.llm_provider}")
            except Exception as e:
                logger.error(f"LangChain {self.llm_provider} LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise
        return self._pandas_llm

    def _create_pandas_agent(self, df: pd.DataFrame):
        """LangChain pandas agent ìƒì„±"""
        try:
            if not LANGCHAIN_AVAILABLE:
                logger.error("LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ - fallback ëª¨ë“œ ì‚¬ìš©")
                return None

            if create_pandas_dataframe_agent is None:
                logger.error("create_pandas_dataframe_agent not available")
                return None

            llm = self._get_pandas_llm()

            agent = create_pandas_dataframe_agent(
                llm,
                df,
                verbose=True,
                # agent_type=ZERO_SHOT_REACT_DESCRIPTION,
                agent_type=TOOL_CALLING_DESCRIPTION,
                handle_parsing_errors=True,
                max_iterations=20,
                max_execution_time=120,
                return_intermediate_steps=True,
                allow_dangerous_code=True,
            )

            logger.info(
                f"Pandas DataFrame Agent ìƒì„± ì„±ê³µ - DataFrame shape: {df.shape}"
            )
            return agent

        except Exception as e:
            logger.error(f"Pandas Agent ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _extract_data_info(self, df: pd.DataFrame) -> Dict[str, Any]:
        """DataFrameì˜ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            return {
                "shape": df.shape,
                "columns": list(df.columns),
                "dtypes": df.dtypes.to_dict(),
                "null_counts": df.isnull().sum().to_dict(),
                "memory_usage": df.memory_usage(deep=True).sum(),
                "sample_data": df.head(3).to_dict() if len(df) > 0 else {},
            }
        except Exception as e:
            logger.error(f"ë°ì´í„° ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    def _generate_insurance_prompt(self, query: str, df_info: Dict) -> str:
        """ë³´í—˜ ë°ì´í„° ë¶„ì„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        base_prompt = f"""
ë‹¹ì‹ ì€ ë³´í—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë³´í—˜ë£Œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¶„ì„ ëª©í‘œ: {query}
ë°ì´í„° ì •ë³´:
- í˜•íƒœ: {df_info.get('shape', 'Unknown')}
- ì»¬ëŸ¼: {df_info.get('columns', [])}
- ë°ì´í„° íƒ€ì…: {df_info.get('dtypes', {})}

ë³´í—˜ ë¶„ì„ ê°€ì´ë“œ:
1. ë³´í—˜ë£Œ ë¹„êµ: ê°€ì¥ ì €ë ´í•œ ë³´í—˜ì‚¬ ìˆœìœ„ ì œì‹œ
2. ë³´ì¥ í•­ëª© ë¶„ì„: ì•”ì§„ë‹¨ë¹„, ìƒí•´ë³´ì¥ ë“± ì£¼ìš” ë³´ì¥ ë¹„êµ  
3. íŠ¹ì§• ë¶„ì„: ê° ë³´í—˜ì‚¬ì˜ ì¥ë‹¨ì  ë° ì°¨ì´ì 
4. í•©ë¦¬ì ì¸ ì¶”ì²œ: ë¹„ìš©-íš¨ê³¼ì„± ê¸°ì¤€ ì¶”ì²œ

ë¶„ì„ ì§€ì¹¨:
- ì „ì²´ ë°ì´í„° ê¸°ë°˜ í†µê³„ì  ë¶„ì„ ìˆ˜í–‰
- íŠ¹ì´ê°’(outlier) í™•ì¸ ë° ë¶„ì„
- ë³´í—˜ì‚¬ë³„ ë³´ì¥ ë‚´ìš© ìƒì„¸ ë¹„êµ
- í•œêµ­ì–´ ë³´í—˜ ìš©ì–´ ì‚¬ìš©
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ë°ì´í„° ì œê³µ
"""

        # ì§ˆë¬¸ ìœ í˜•ë³„ ì¶”ê°€ í”„ë¡¬í”„íŠ¸
        query_lower = query.lower()
        if "ì €ë ´" in query_lower or "ì‹¼" in query_lower or "ê°€ê²©" in query_lower:
            return (
                base_prompt
                + "\n\níŠ¹íˆ ë³´í—˜ë£Œ í•©ê³„ê°€ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³ , ê°€ì„±ë¹„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."
            )
        elif (
            "ë³´ì¥" in query_lower or "ë‹´ë³´" in query_lower or "ë³´ì¥ë‚´ìš©" in query_lower
        ):
            return (
                base_prompt
                + "\n\nê° ë³´ì¥ í•­ëª©ë³„ ìƒì„¸ ë¹„êµì™€ ë³´ì¥ ë‚´ìš©ì˜ ì°¨ì´ì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
            )
        elif "ì¶”ì²œ" in query_lower or "ì–´ë–¤" in query_lower:
            return (
                base_prompt
                + "\n\nê³ ê°ì˜ ì…ì¥ì—ì„œ ê°€ì¥ í•©ë¦¬ì ì¸ ì„ íƒì„ ì¶”ì²œí•˜ê³  ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            )
        elif "ë¹„êµ" in query_lower or "ì°¨ì´" in query_lower:
            return base_prompt + "\n\në³´í—˜ì‚¬ë³„ ì°¨ì´ì ì„ ëª…í™•í•˜ê²Œ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”."

        return base_prompt

    def _execute_fallback_analysis(
        self, df: pd.DataFrame, query: str
    ) -> Dict[str, Any]:
        """LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œì˜ fallback ë¶„ì„"""
        start_time = time.time()

        try:
            logger.info("Fallback ë¶„ì„ ëª¨ë“œ ì‹¤í–‰ - í†µê³„ì  ë¶„ì„ ìˆ˜í–‰")

            # ê¸°ë³¸ í†µê³„ ì •ë³´ ê³„ì‚°
            df_info = self._extract_data_info(df)
            analysis_prompt = self._generate_insurance_prompt(query, df_info)

            # ë°ì´í„° í†µê³„ ë¶„ì„
            stats = {}
            try:
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0:
                    stats = df.describe().to_dict()
                    logger.info("ê¸°ìˆ  í†µê³„ ê³„ì‚° ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"ê¸°ìˆ  í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")

            # ë³´í—˜ì‚¬ë³„ ìš”ì•½ (ê°€ëŠ¥í•œ ê²½ìš°)
            company_summary = {}
            for col in df.columns:
                if any(
                    keyword in col.lower()
                    for keyword in ["ë³´í—˜ì‚¬", "company", "insurer"]
                ):
                    try:
                        company_summary[col] = df[col].value_counts().to_dict()
                    except:
                        pass

            # ë³´í—˜ë£Œ ê´€ë ¨ ë¶„ì„ (ê°€ëŠ¥í•œ ê²½ìš°)
            premium_analysis = {}
            try:
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                for col in numeric_cols:
                    if any(
                        keyword in col.lower()
                        for keyword in ["ë³´í—˜ë£Œ", "premium", "ê¸ˆì•¡", "amount"]
                    ):
                        try:
                            premium_analysis[col] = {
                                "mean": float(df[col].mean()),
                                "min": float(df[col].min()),
                                "max": float(df[col].max()),
                                "std": float(df[col].std()),
                            }
                        except:
                            pass
            except:
                pass

            # ë¶„ì„ ê²°ê³¼ êµ¬ì„±
            analysis_result = f"""
ë°ì´í„° í†µê³„ ë¶„ì„ ê²°ê³¼:

## ê¸°ë³¸ ì •ë³´
- ë°ì´í„° í˜•íƒœ: {df.shape}
- ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}

## ìˆ˜ì¹˜í˜• ë°ì´í„° ìš”ì•½
{df.describe().to_string() if df.select_dtypes(include=[np.number]).shape[1] > 0 else 'ìˆ˜ì¹˜í˜• ë°ì´í„° ì—†ìŒ'}

## ë³´í—˜ì‚¬ë³„ í˜„í™©
{chr(10).join([f'- {k}: {v}' for k, v in company_summary.items()]) if company_summary else 'ë³´í—˜ì‚¬ ì •ë³´ ì—†ìŒ'}

## ë³´í—˜ë£Œ ê´€ë ¨ í†µê³„
{chr(10).join([f'- {k}: í‰ê·  {v["mean"]:,.0f}, ìµœì†Œ {v["min"]:,.0f}, ìµœëŒ€ {v["max"]:,.0f}' for k, v in premium_analysis.items()]) if premium_analysis else 'ë³´í—˜ë£Œ ì •ë³´ ì—†ìŒ'}

## ë¶„ì„ ì œì•ˆ
ê³ ê° ì§ˆë¬¸ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì€ ì¶”ê°€ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:
1. íŠ¹ì • ë³´í—˜ì‚¬ë³„ ìƒì„¸ ë¹„êµ
2. ë³´í—˜ë£Œ ìˆ˜ì¤€ë³„ ìˆœìœ„ ë¶„ì„
3. ë³´ì¥ í•­ëª©ë³„ ì°¨ì´ì  ë¶„ì„
"""

            duration = time.time() - start_time
            logger.info(f"Fallback ë¶„ì„ ì™„ë£Œ - ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")

            return {
                "status": "success",
                "analysis": analysis_result,
                "steps": [("fallback_analysis", "í†µê³„ì  ë¶„ì„ ìˆ˜í–‰")],
                "duration": duration,
                "mode": "fallback",
            }

        except Exception as e:
            logger.error(f"Fallback ë¶„ì„ ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
            return {"status": "error", "message": str(e)}

    def _validate_agent_result(self, result: Any) -> Dict[str, Any]:
        """Agent ì‹¤í–‰ ê²°ê³¼ ìœ íš¨ì„± ê²€ì‚¬ ë° í‘œì¤€í™”"""
        try:
            # ê²°ê³¼ í˜•ì‹ ê²€ì‚¬
            if not isinstance(result, dict):
                logger.warning(f"Agent ê²°ê³¼ê°€ dict í˜•ì‹ì´ ì•„ë‹˜: {type(result)}")
                return {"status": "invalid_format", "message": "ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜"}

            # output í•„ë“œ ê²€ì‚¬
            if "output" not in result:
                logger.warning("Agent ê²°ê³¼ì— output í•„ë“œ ì—†ìŒ")
                logger.debug(f"Agent ê²°ê³¼ í‚¤: {list(result.keys())}")
                return {"status": "missing_output", "message": "Output í•„ë“œ ëˆ„ë½"}

            agent_output = result["output"]

            # output ë‚´ìš© ê²€ì‚¬
            if not agent_output or not str(agent_output).strip():
                logger.warning("Agent outputì´ ë¹„ì–´ìˆìŒ")
                return {"status": "empty_output", "message": "ë¶„ì„ ê²°ê³¼ ì—†ìŒ"}

            # Agent outputì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
            output_str = ""
            if isinstance(agent_output, list):
                # ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©ì—ì„œ text í•„ë“œ ì¶”ì¶œ
                for item in agent_output:
                    if isinstance(item, dict) and "text" in item:
                        output_str += item["text"]
                    elif isinstance(item, str):
                        output_str += item
            else:
                output_str = str(agent_output)
            if len(output_str.strip()) < 5:
                logger.warning(f"Agent outputì´ ë„ˆë¬´ ì§§ìŒ: {len(output_str)}ì")
                return {"status": "too_short", "message": "ë¶„ì„ ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìŒ"}

            # í•œêµ­ì–´ ë‚´ìš© ê²€ì‚¬
            if not any(ord(char) > 127 for char in output_str):
                logger.warning("Agent outputì— í•œêµ­ì–´ ë‚´ìš© ì—†ìŒ")
                # ì˜ì–´ë§Œ ìˆì–´ë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬ (fallback ë°©ì§€)
                logger.info("ì˜ì–´ ì‘ë‹µì´ì§€ë§Œ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬")

            # ìµœì¢… ìœ íš¨ì„± ê²€ì‚¬ í†µê³¼
            steps = result.get("intermediate_steps", [])

            return {
                "status": "success",
                "analysis": output_str,
                "steps": steps,
                "intermediate_steps_count": len(steps),
                "output_length": len(output_str),
                "result_keys": list(result.keys()),
                "validation_time": time.time(),
            }

        except Exception as e:
            logger.error(f"Agent ê²°ê³¼ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
            return {
                "status": "validation_error",
                "message": f"ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}",
            }

    def _execute_agent_analysis(self, df: pd.DataFrame, query: str) -> Dict[str, Any]:
        """LangChain agentë¥¼ í†µí•œ ë°ì´í„° ë¶„ì„ ì‹¤í–‰ - ê°œì„ ëœ ë²„ì „"""
        start_time = time.time()

        # LangChain ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ì²´í¬
        if not LANGCHAIN_AVAILABLE:
            logger.info("LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ Fallback ëª¨ë“œë¡œ ì „í™˜")
            return self._execute_fallback_analysis(df, query)

        try:
            # Agent ìƒì„±
            logger.info("LangChain Pandas Agent ìƒì„± ì‹œì‘")
            agent = self._create_pandas_agent(df)

            if agent is None:
                logger.warning("Agent ìƒì„± ì‹¤íŒ¨ - Fallback ëª¨ë“œë¡œ ì „í™˜")
                return self._execute_fallback_analysis(df, query)

            # ë³´í—˜ íŠ¹í™” í”„ë¡¬í”„íŠ¸
            df_info = self._extract_data_info(df)
            analysis_prompt = self._generate_insurance_prompt(query, df_info)

            logger.info(f"Agent ë¶„ì„ ì‹¤í–‰ ì‹œì‘ - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(analysis_prompt)}")

            # Agent ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ ì„¤ì •
            agent_start_time = time.time()
            result = agent.invoke({"input": analysis_prompt})
            agent_execution_time = time.time() - agent_start_time

            logger.info(f"Agent ì‹¤í–‰ ì™„ë£Œ - ì†Œìš” ì‹œê°„: {agent_execution_time:.2f}ì´ˆ")

            # ê²°ê³¼ ìœ íš¨ì„± ê²€ì‚¬ ë° í‘œì¤€í™”
            validated_result = self._validate_agent_result(result)

            if validated_result["status"] == "success":
                # ì„±ê³µ ë¡œê¹…
                logger.info(
                    f"Agent ë¶„ì„ ì„±ê³µ - ê²°ê³¼ ê¸¸ì´: {validated_result['output_length']}ì, "
                    f"ë‹¨ê³„ ìˆ˜: {validated_result['intermediate_steps_count']}, "
                    f"ì‹¤í–‰ ì‹œê°„: {agent_execution_time:.2f}ì´ˆ"
                )

                # ìƒì„¸ ë¶„ì„ ë‹¨ê³„ ë¡œê¹… (ì„ íƒì )
                if validated_result["intermediate_steps_count"] > 0:
                    logger.debug("=== Agent ë¶„ì„ ë‹¨ê³„ ìƒì„¸ ===")
                    for i, step in enumerate(
                        validated_result["steps"][:3]
                    ):  # ì²˜ìŒ 3ë‹¨ê³„ë§Œ ë¡œê¹…
                        logger.debug(f"ë‹¨ê³„ {i+1}: {str(step)[:100]}...")

                # í‘œì¤€í™”ëœ ê²°ê³¼ ë°˜í™˜
                return {
                    "status": "success",
                    "analysis": validated_result["analysis"],
                    "steps": validated_result["steps"],
                    "duration": time.time() - start_time,
                    "mode": "langchain_agent",
                    "execution_time": agent_execution_time,
                    "validation_info": {
                        "output_length": validated_result["output_length"],
                        "steps_count": validated_result["intermediate_steps_count"],
                    },
                }
            else:
                # ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨ - ìƒì„¸ ë¡œê·¸ì™€ í•¨ê»˜ fallback
                logger.error(
                    f"Agent ê²°ê³¼ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {validated_result['status']} - {validated_result['message']}"
                )
                logger.debug(f"Agent ì›ë³¸ ê²°ê³¼: {str(result)[:500]}...")

                # íŠ¹ì • ì‹¤íŒ¨ ìœ í˜•ë³„ ì²˜ë¦¬
                if validated_result["status"] in ["too_short", "empty_output"]:
                    logger.warning(
                        "Agentê°€ ë¶ˆì¶©ë¶„í•œ ì‘ë‹µì„ ì œê³µ - Fallback ëª¨ë“œë¡œ ì „í™˜"
                    )
                elif validated_result["status"] in ["invalid_format", "missing_output"]:
                    logger.error("Agent ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜ - Fallback ëª¨ë“œë¡œ ì „í™˜")
                else:
                    logger.warning(
                        f"ê¸°íƒ€ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {validated_result['message']}"
                    )

                return self._execute_fallback_analysis(df, query)

        except Exception as e:
            error_time = time.time() - start_time
            logger.error(
                f"Agent ì‹¤í–‰ ì¹˜ëª…ì  ì˜¤ë¥˜: {type(e).__name__}: {str(e)} - "
                f"ì†Œìš” ì‹œê°„: {error_time:.2f}ì´ˆ - Fallback ëª¨ë“œë¡œ ì „í™˜"
            )

            # ì—ëŸ¬ íƒ€ì…ë³„ ìƒì„¸ ì²˜ë¦¬
            error_str = str(e).lower()
            if "timeout" in error_str or "ì‹œê°„ ì´ˆê³¼" in error_str:
                logger.error("Agent ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼")
            elif "parsing" in error_str or "íŒŒì‹±" in error_str:
                logger.error("Agent ì¶œë ¥ íŒŒì‹± ì˜¤ë¥˜")
            elif "memory" in error_str or "ë©”ëª¨ë¦¬" in error_str:
                logger.error("Agent ì‹¤í–‰ ë©”ëª¨ë¦¬ ì˜¤ë¥˜")

            return self._execute_fallback_analysis(df, query)

    def _generate_final_analysis(
        self, agent_result: Dict, query: str, df: pd.DataFrame
    ) -> str:
        """ìµœì¢… LLMì„ í†µí•œ ì¢…í•© ë¶„ì„"""
        start_time = time.time()

        try:
            if agent_result["status"] != "success":
                return f"ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {agent_result['message']}"

            # ìµœì¢… ì¢…í•© í”„ë¡¬í”„íŠ¸
            final_prompt = f"""
ë‹¤ìŒì€ LangChain pandas agentì˜ ë³´í—˜ ë°ì´í„° ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

{agent_result['analysis']}

ê³ ê° ì§ˆë¬¸: {query}

ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë³´í—˜ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ì¢…í•©ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. **í•µì‹¬ ë¶„ì„ ë‚´ìš© ìš”ì•½**: ê°€ì¥ ì¤‘ìš”í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½
2. **ë³´í—˜ì‚¬ë³„ íŠ¹ì§• ë¹„êµ**: ê° ë³´í—˜ì‚¬ì˜ ì¥ì , ë‹¨ì , ì°¨ì´ì  ëª…í™•íˆ ë¹„êµ  
3. **ìˆ˜ì¹˜ ê¸°ë°˜ ì¶”ì²œ**: êµ¬ì²´ì ì¸ ê¸ˆì•¡ê³¼ ë°ì´í„°ë¥¼ ê·¼ê±°ë¡œ ì¶”ì²œ
4. **ì‹¤ì§ˆì ì¸ ì¡°ì–¸**: ê³ ê°ì˜ ì…ì¥ì—ì„œ ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ì´ ë  ì •ë³´ ì œê³µ

ë‹µë³€ í˜•ì‹:
- ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ ì‚¬ìš©
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ë°ì´í„° í¬í•¨  
- ë¶ˆë › í¬ì¸íŠ¸ë‚˜ ë²ˆí˜¸ë¡œ êµ¬ì¡°í™”
- ì „ë¬¸ê°€ì ì´ë©´ì„œ ì¹œì ˆí•œ í†¤
"""

            logger.info(f"ìµœì¢… ë¶„ì„ ìƒì„± ì‹œì‘ - LLM í˜¸ì¶œ")

            # ìµœì¢… LLM ì‘ë‹µ ìƒì„±
            response = self.llm(model="gemini-3-flash-preview", contents=[final_prompt])

            result_text = "ë¶„ì„ ê²°ê³¼ ìƒì„± ì‹¤íŒ¨"
            if response and hasattr(response, "text") and response.text:
                result_text = response.text

            return result_text

        except Exception as e:
            logger.error(f"ìµœì¢… ë¶„ì„ ìƒì„± ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
            return f"ìµœì¢… ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def _optimize_dataframe_memory(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„°í”„ë ˆì„ ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            original_memory = df.memory_usage(deep=True).sum()

            # ìˆ˜ì¹˜í˜• ë°ì´í„° ìµœì í™”
            for col in df.select_dtypes(include=["int64"]).columns:
                df[col] = pd.to_numeric(df[col], downcast="integer")

            for col in df.select_dtypes(include=["float64"]).columns:
                df[col] = pd.to_numeric(df[col], downcast="float")

            # ë¬¸ìì—´ ë°ì´í„° ìµœì í™”
            for col in df.select_dtypes(include=["object"]).columns:
                if df[col].nunique() / len(df) < 0.5:  # ì¹´ë””ë„ë¦¬í‹°ê°€ ë‚®ì€ ê²½ìš°
                    df[col] = df[col].astype("category")

            optimized_memory = df.memory_usage(deep=True).sum()
            memory_reduction = (
                (original_memory - optimized_memory) / original_memory * 100
            )

            logger.info(
                f"ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ - {original_memory/1024/1024:.2f}MB â†’ {optimized_memory/1024/1024:.2f}MB ({memory_reduction:.1f}% ê°ì†Œ)"
            )

            return df

        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return df

    def pandas_analysis(
        self,
        df: pd.DataFrame,
        query: str,
        comparison_table: Optional[pd.DataFrame] = None,
    ) -> str:
        """
        ê°œì„ ëœ ë³´í—˜ë£Œ ë°ì´í„° ë¶„ì„ - LangChain pandas agent í†µí•©
        ê°€ë³€ ë‹¨ê³„ êµ¬ì¡°: 1) ë°ì´í„° ì¤€ë¹„ â†’ 2) Pandas Agent ë¶„ì„ â†’ 3) ìµœì¢… LLM ì¢…í•© (ì„ íƒì )
        """
        start_time = time.time()
        logger.info(f"=== {self.pandas_analysis_stages}ë‹¨ê³„ Pandas ë¶„ì„ ì‹œì‘ ===")
        logger.info(
            f"ì¿¼ë¦¬: '{query}', DataFrame í˜•íƒœ: {df.shape if df is not None else 'None'}"
        )

        try:
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            if df is None or df.empty:
                logger.warning("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

            # 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„
            prep_start = time.time()
            logger.info("1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ì‹œì‘")

            # íŒë‹¤ìŠ¤ ì—ì´ì „íŠ¸ ë¶„ì„ì„ ìœ„í•´ aggregated_df ì‚¬ìš© (ë³´ì¥ê¸ˆì•¡ ì •ë³´ ë³´ì¡´)
            if comparison_table is not None and not comparison_table.empty:
                # comparison_tableì´ ìˆë”ë¼ë„ aggregated_dfë¥¼ ìš°ì„  ì‚¬ìš©
                logger.info(
                    "comparison_tableì´ ìˆì§€ë§Œ aggregated_dfë¡œ ë¶„ì„ ì§„í–‰ (ë³´ì¥ê¸ˆì•¡ ì •ë³´ ë³´ì¡´)"
                )
                try:
                    from data_manager import data_manager

                    if (
                        data_manager.coverage_premiums_df is not None
                        and not data_manager.coverage_premiums_df.empty
                    ):
                        # ë™ì ìœ¼ë¡œ aggregated_df ìƒì„±
                        logger.info("ë³´í—˜ë£Œ ë°ì´í„° ì •ê·œí™” ì‹œì‘")
                        normalized_df = data_manager.normalize_coverage_amounts(
                            data_manager.coverage_premiums_df
                        )

                        logger.info("ë³´í—˜ì‚¬ë³„ ë°ì´í„° ì§‘ê³„ ì‹œì‘")
                        analysis_df = data_manager.aggregate_coverage_by_code(
                            normalized_df
                        )
                        data_type = "ì§‘ê³„ëœ ë°ì´í„°í”„ë ˆì„ (aggregated_df)"
                        logger.info(
                            f"ì§‘ê³„ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ - í˜•íƒœ: {analysis_df.shape}"
                        )
                    else:
                        analysis_df = df
                        data_type = "ì›ë³¸ ë°ì´í„°"
                        logger.info("ì›ë³¸ ë°ì´í„°ë¡œ ë¶„ì„ ì§„í–‰")
                except Exception as e:
                    logger.error(f"ì§‘ê³„ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹¤íŒ¨: {e}")
                    analysis_df = df
                    data_type = "ì›ë³¸ ë°ì´í„°(ì§‘ê³„ ì‹¤íŒ¨)"
            else:
                # ë¹„êµ í‘œê°€ ì—†ëŠ” ê²½ìš° aggregated_df ìƒì„± ì‹œë„
                logger.info("ë™ì  ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹œë„")
                try:
                    from data_manager import data_manager

                    if (
                        data_manager.coverage_premiums_df is not None
                        and not data_manager.coverage_premiums_df.empty
                    ):
                        # ë™ì ìœ¼ë¡œ aggregated_df ìƒì„±
                        logger.info("ë³´í—˜ë£Œ ë°ì´í„° ì •ê·œí™” ì‹œì‘")
                        normalized_df = data_manager.normalize_coverage_amounts(
                            data_manager.coverage_premiums_df
                        )

                        logger.info("ë³´í—˜ì‚¬ë³„ ë°ì´í„° ì§‘ê³„ ì‹œì‘")
                        analysis_df = data_manager.aggregate_coverage_by_code(
                            normalized_df
                        )
                        data_type = "ë™ì  ìƒì„± ì§‘ê³„ ë°ì´í„°í”„ë ˆì„"
                        logger.info(
                            f"ë™ì  ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ - í˜•íƒœ: {analysis_df.shape}"
                        )
                    else:
                        analysis_df = df
                        data_type = "ì›ë³¸ ë°ì´í„°"
                        logger.info("ì›ë³¸ ë°ì´í„°ë¡œ ë¶„ì„ ì§„í–‰")
                except Exception as e:
                    logger.error(f"ë™ì  ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹¤íŒ¨: {e}")
                    analysis_df = df
                    data_type = "ì›ë³¸ ë°ì´í„°(ì§‘ê³„ ì‹¤íŒ¨)"

            # ë©”ëª¨ë¦¬ ìµœì í™”
            analysis_df = self._optimize_dataframe_memory(analysis_df)

            prep_time = time.time() - prep_start
            logger.info(
                f"1ë‹¨ê³„ ì™„ë£Œ: ë°ì´í„° ì¤€ë¹„ - ìœ í˜•: {data_type}, ì†Œìš” ì‹œê°„: {prep_time:.2f}ì´ˆ"
            )

            # 2ë‹¨ê³„: LangChain Pandas Agent ë¶„ì„
            logger.info("2ë‹¨ê³„: Pandas Agent ë¶„ì„ ì‹œì‘")
            agent_start = time.time()

            agent_result = self._execute_agent_analysis(analysis_df, query)

            agent_time = time.time() - agent_start
            logger.info(
                f"2ë‹¨ê³„ ì™„ë£Œ: Agent ë¶„ì„ - ìƒíƒœ: {agent_result['status']}, ì†Œìš” ì‹œê°„: {agent_time:.2f}ì´ˆ"
            )

            # 3ë‹¨ê³„: ìµœì¢… LLM ì¢…í•© ë¶„ì„ (ë‹¨ê³„ ìˆ˜ ì„¤ì •ì— ë”°ë¼ ì‹¤í–‰)

            # Agent ê²°ê³¼ ì•ˆì „ ì¶”ì¶œ - ìƒˆë¡œìš´ ìœ íš¨ì„± ê²€ì‚¬ ê²°ê³¼ êµ¬ì¡° ë°˜ì˜
            if agent_result.get("status") == "success":
                final_result = agent_result.get("analysis", "")
                # ì„±ê³µ ì‹œ ì¶”ê°€ ì •ë³´ ë¡œê¹…
                validation_info = agent_result.get("validation_info", {})
                if validation_info:
                    logger.info(
                        f"Agent ë¶„ì„ ìƒì„¸ - ê¸¸ì´: {validation_info.get('output_length', 0)}ì, "
                        f"ë‹¨ê³„: {validation_info.get('steps_count', 0)}ê°œ"
                    )
            else:
                # Agent ì‹¤íŒ¨ ì‹œ fallback ê²°ê³¼ ì‚¬ìš©
                logger.warning(
                    f"Agent ë¶„ì„ ì‹¤íŒ¨: {agent_result.get('status', 'unknown')}"
                )
                final_result = agent_result.get(
                    "analysis", "ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )

            if self.pandas_analysis_stages >= 3:
                logger.info("3ë‹¨ê³„: ìµœì¢… LLM ì¢…í•© ë¶„ì„ ì‹œì‘")
                final_start = time.time()

                final_result = self._generate_final_analysis(
                    agent_result, query, analysis_df
                )

                final_time = time.time() - final_start
                logger.info(f"3ë‹¨ê³„ ì™„ë£Œ: ìµœì¢… ë¶„ì„ - ì†Œìš” ì‹œê°„: {final_time:.2f}ì´ˆ")
            else:
                logger.info(
                    f"3ë‹¨ê³„ ê±´ë„ˆëœ€ - ì„¤ì •ëœ ë‹¨ê³„ ìˆ˜: {self.pandas_analysis_stages}"
                )

            total_time = time.time() - start_time
            logger.info(
                f"=== ì „ì²´ ë¶„ì„ ì™„ë£Œ ({self.pandas_analysis_stages}ë‹¨ê³„) === ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ, ê²°ê³¼ ê¸¸ì´: {len(final_result)}ì"
            )

            return final_result

        except Exception as e:
            error_time = time.time() - start_time
            logger.error(f"pandas_analysis ì¹˜ëª…ì  ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ ì‹œì : {error_time:.2f}ì´ˆ")
            logger.error(f"ë°ì´í„° ì •ë³´: shape={df.shape if df is not None else 'None'}")

            # ì‚¬ìš©ì ì¹œí™”ì  ì—ëŸ¬ ë©”ì‹œì§€
            error_message = self._generate_user_friendly_error(e)
            return error_message

    def _generate_user_friendly_error(self, error: Exception) -> str:
        """ì‚¬ìš©ì ì¹œí™”ì  ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±"""
        error_str = str(error).lower()
        error_type = type(error).__name__

        if "timeout" in error_str or "ì‹œê°„" in error_str:
            return "â° ë¶„ì„ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°ê°€ ë„ˆë¬´ ë§ê±°ë‚˜ ë³µì¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "memory" in error_str or "ë©”ëª¨ë¦¬" in error_str:
            return "ğŸ’¾ ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì•„ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¼ë¶€ ë°ì´í„°ë§Œ ë‹¤ì‹œ ë¶„ì„í•´ì£¼ì„¸ìš”."
        elif "api" in error_str or "ì—°ê²°" in error_str:
            return "ğŸ”Œ ì™¸ë¶€ API ì—°ê²°ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "parse" in error_str or "í˜•ì‹" in error_str:
            return (
                "ğŸ“‹ ë°ì´í„° í˜•ì‹ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            )
        else:
            return f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(error)} (ì˜¤ë¥˜ íƒ€ì…: {error_type})"

    def hybrid_chat_with_data(
        self, query: str, llm_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ì „ë‹¬ë°›ì€ LLM ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì§ˆì˜ì‘ë‹µ - DataFrame ìƒì„± ì—†ì´ ì§ì ‘ ì²˜ë¦¬
        """
        start_time = time.time()
        logger.info(f"Hybrid Chat with Data ì‹œì‘ - ì¿¼ë¦¬: '{query}'")
        logger.info(f"LLM ë°ì´í„° í¬ê¸°: {len(llm_data)}")

        try:
            # 1. LLM ë°ì´í„°ë¥¼ í†µí•œ ì§ì ‘ ë¶„ì„
            logger.info("1ë‹¨ê³„: LLM ë°ì´í„° ë¶„ì„ ì‹œì‘")
            analysis_start = time.time()

            # LLM ë°ì´í„°ë¥¼ ë¶„ì„ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            analysis_result = self._analyze_llm_data(query, llm_data)

            analysis_time = time.time() - analysis_start
            logger.info(
                f"1ë‹¨ê³„ ì™„ë£Œ: LLM ë°ì´í„° ë¶„ì„ - ì†Œìš” ì‹œê°„: {analysis_time:.2f}ì´ˆ"
            )

            # 2. ì¢…í•© ì‘ë‹µ ìƒì„±
            logger.info("2ë‹¨ê³„: ì¢…í•© ì‘ë‹µ ìƒì„±")
            response_start = time.time()

            # ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„±
            final_response = self._generate_final_response_with_data_simple(
                query, analysis_result, llm_data
            )

            response_time = time.time() - response_start
            total_time = time.time() - start_time
            logger.info(
                f"2ë‹¨ê³„ ì™„ë£Œ: ì¢…í•© ì‘ë‹µ ìƒì„± - ì†Œìš” ì‹œê°„: {response_time:.2f}ì´ˆ"
            )
            logger.info(f"ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")

            return {
                "response": final_response,
                "data_analysis_available": True,
                "processing_time": total_time,
                "analysis_result": analysis_result,
            }

        except Exception as e:
            error_msg = f"Hybrid Chat with Data ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            logger.error(error_msg)
            return {
                "response": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "data_analysis_available": False,
                "processing_time": time.time() - start_time,
                "error": error_msg,
            }

    def _analyze_llm_data(self, query: str, llm_data: Dict[str, Any]) -> str:
        """LLM ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ"""
        try:
            analysis_parts = []

            # íšŒì‚¬ë³„ ë³´ì¥ ì •ë³´ ë¶„ì„
            for company_key, coverages in llm_data.items():
                if isinstance(coverages, list):
                    company_name = company_key.split("_")[0]  # íšŒì‚¬ëª… ì¶”ì¶œ
                    analysis_parts.append(f"## {company_name} ë³´ì¥ ì •ë³´:")

                    for coverage in coverages:
                        coverage_name = coverage.get("coverage_name", "ì•Œ ìˆ˜ ì—†ëŠ” ë³´ì¥")
                        coverage_code = coverage.get("coverage_code", "")
                        premium = coverage.get("sum_premium", 0)
                        max_amount = coverage.get("guide_contract_amount_max", 0)

                        analysis_parts.append(
                            f"- {coverage_name}({coverage_code}): ë³´í—˜ë£Œ {premium:,}ì›, ìµœëŒ€ ë³´ì¥ {max_amount:,}ì›"
                        )

                    analysis_parts.append("")

            # ì „ì²´ ìš”ì•½
            total_companies = len(llm_data)
            total_coverages = sum(
                len(coverages) if isinstance(coverages, list) else 0
                for coverages in llm_data.values()
            )
            analysis_parts.append(f"## ì „ì²´ ìš”ì•½")
            analysis_parts.append(f"- ì´ {total_companies}ê°œ ë³´í—˜ì‚¬")
            analysis_parts.append(f"- ì´ {total_coverages}ê°œ ë³´ì¥ í•­ëª©")

            return "\n".join(analysis_parts)

        except Exception as e:
            logger.error(f"LLM ë°ì´í„° ë¶„ì„ ì˜¤ë¥˜: {e}")
            return "ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def _generate_final_response_with_data_simple(
        self,
        query: str,
        analysis_result: str,
        llm_data: Dict[str, Any],
    ) -> str:
        """ì „ë‹¬ë°›ì€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„± (ë²¡í„° ê²€ìƒ‰ ì—†ìŒ)"""
        try:
            # LangChainì´ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
            if LANGCHAIN_AVAILABLE and self._pandas_llm:
                # ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = f"""
                ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë³´í—˜ë£Œ ë¹„êµ ë°ì´í„° ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

                **ì‚¬ìš©ì ì§ˆë¬¸:** {query}

                **ë³´í—˜ ë°ì´í„° ë¶„ì„:**
                {analysis_result}

                ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. 
                êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë¹„êµ ë¶„ì„ì„ í¬í•¨í•´ì£¼ì„¸ìš”.
                """

                response = self._pandas_llm.invoke(prompt)
                return response.content if hasattr(response, 'content') else str(response)
            else:
                # LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
                return self._generate_simple_response_with_data(
                    query, analysis_result, llm_data
                )

        except Exception as e:
            logger.error(f"ìµœì¢… ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return self._generate_simple_response_with_data(
                query, analysis_result, llm_data
            )



    def _generate_simple_response_with_data(
        self, query: str, analysis_result: str, llm_data: Dict[str, Any]
    ) -> str:
        """LangChain ì—†ì´ ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±"""
        try:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì‘ë‹µ
            query_lower = query.lower()

            if "ê°€ì¥ ì €ë ´" in query_lower or "ìŒ‰" in query_lower or "ì‹¼" in query_lower:
                # ìµœì € ë³´í—˜ë£Œ íšŒì‚¬ ì°¾ê¸°
                cheapest_company = None
                cheapest_premium = float("inf")

                for company_key, coverages in llm_data.items():
                    if isinstance(coverages, list):
                        total_premium = sum(
                            coverage.get("sum_premium", 0) for coverage in coverages
                        )
                        if total_premium < cheapest_premium:
                            cheapest_premium = total_premium
                            cheapest_company = company_key.split("_")[0]

                if cheapest_company:
                    return f"ê°€ì¥ ì €ë ´í•œ ë³´í—˜ì‚¬ëŠ” **{cheapest_company}**ì´ë©°, ì´ ë³´í—˜ë£ŒëŠ” {cheapest_premium:,.0f}ì›ì…ë‹ˆë‹¤.\n\n{analysis_result}"

            elif "ë³´ì¥" in query_lower or "í•­ëª©" in query_lower:
                return f"ë³´ì¥ í•­ëª©ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:\n\n{analysis_result}"

            else:
                return f"ì§ˆë¬¸ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:\n\n{analysis_result}"

        except Exception as e:
            logger.error(f"ê°„ë‹¨ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼:\n{analysis_result}"

    async def hybrid_chat_stream_with_data(
        self, query: str, llm_data: Dict[str, Any]
    ) -> Any:
        """
        ì „ë‹¬ë°›ì€ LLM ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ì§ˆì˜ì‘ë‹µ
        """
        start_time = time.time()
        logger.info(f"Hybrid Chat Stream with Data ì‹œì‘ - ì¿¼ë¦¬: '{query}'")
        logger.info(f"LLM ë°ì´í„° í¬ê¸°: {len(llm_data)}")

        try:
            # 1. LLM ë°ì´í„° ë¶„ì„ ì‹œì‘
            logger.info("1ë‹¨ê³„: LLM ë°ì´í„° ë¶„ì„ ì‹œì‘")
            yield {
                "status": "analyzing",
                "message": "ë³´í—˜ ë°ì´í„° ë¶„ì„ ì¤‘...",
                "progress": 40,
                "timestamp": time.time(),
            }

            analysis_start = time.time()

            # LLM ë°ì´í„°ë¥¼ ë¶„ì„ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            analysis_result = self._analyze_llm_data(query, llm_data)

            analysis_time = time.time() - analysis_start
            logger.info(
                f"1ë‹¨ê³„ ì™„ë£Œ: LLM ë°ì´í„° ë¶„ì„ - ì†Œìš” ì‹œê°„: {analysis_time:.2f}ì´ˆ"
            )

            yield {
                "status": "analyzing",
                "message": f"ë°ì´í„° ë¶„ì„ ì™„ë£Œ (ì´ {len(llm_data)}ê°œì‚¬)",
                "progress": 70,
                "timestamp": time.time(),
            }

            # 2. ì¢…í•© ì‘ë‹µ ìƒì„±
            logger.info("2ë‹¨ê³„: ì¢…í•© ì‘ë‹µ ìƒì„±")
            yield {
                "status": "finalizing",
                "message": "ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘...",
                "progress": 90,
                "timestamp": time.time(),
            }

            response_start = time.time()

            # ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„±
            final_response = self._generate_final_response_with_data_simple(
                query, analysis_result, llm_data
            )

            response_time = time.time() - response_start
            total_time = time.time() - start_time
            logger.info(
                f"2ë‹¨ê³„ ì™„ë£Œ: ì¢…í•© ì‘ë‹µ ìƒì„± - ì†Œìš” ì‹œê°„: {response_time:.2f}ì´ˆ"
            )
            logger.info(f"ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")

            # ìµœì¢… ì‘ë‹µ ì „ì†¡
            yield {
                "status": "complete",
                "message": "ë¶„ì„ ì™„ë£Œ!",
                "progress": 100,
                "response": final_response,
                "data_analysis_available": True,
                "processing_time": total_time,
                "analysis_result": analysis_result,
                "timestamp": time.time(),
            }

        except Exception as e:
            error_msg = f"Hybrid Chat Stream with Data ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            logger.error(error_msg)
            yield {
                "status": "error",
                "message": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "data_analysis_available": False,
                "processing_time": time.time() - start_time,
                "error": error_msg,
                "timestamp": time.time(),
            }

    def hybrid_chat(
        self, query: str, df: pd.DataFrame, insurance_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ë°ì´í„° ë¶„ì„ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ - ë¹„êµ í‘œ í™œìš©
        """
        start_time = time.time()
        logger.info(f"Hybrid Chat ì‹œì‘ - ì¿¼ë¦¬: '{query}'")
        logger.info(
            f"ì…ë ¥ ë°ì´í„° - DataFrame í˜•íƒœ: {df.shape if df is not None else 'None'}, ë³´í—˜ ë°ì´í„° ìˆ˜: {len(insurance_data) if insurance_data else 0}"
        )

        try:
            # 1. Pandas ë°ì´í„° ë¶„ì„

            # 2. ë¹„êµ í‘œ ìƒì„± ë° Pandas ë°ì´í„° ë¶„ì„
            pandas_result = ""

            if df is not None and not df.empty:
                # ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹œë„
                from data_manager import data_manager

                try:
                    if (
                        data_manager.coverage_premiums_df is not None
                        and not data_manager.coverage_premiums_df.empty
                    ):
                        # ë™ì ìœ¼ë¡œ ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± (ë³´ì¥ê¸ˆì•¡ ì •ë³´ ë³´ì¡´)
                        logger.info("í•˜ì´ë¸Œë¦¬ë“œ ì±—ì—ì„œ ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹œì‘")
                        normalized_df = data_manager.normalize_coverage_amounts(
                            data_manager.coverage_premiums_df
                        )
                        aggregated_df = data_manager.aggregate_coverage_by_code(
                            normalized_df
                        )
                        logger.info(
                            "ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ - pandas_analysis í˜¸ì¶œ"
                        )

                        # ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ì„ ì‚¬ìš©í•œ ë¶„ì„
                        pandas_result = self.pandas_analysis(df, query, aggregated_df)
                    else:
                        pandas_result = self.pandas_analysis(df, query)
                except Exception as e:
                    logger.warning(
                        f"Failed to create aggregated dataframe for analysis: {e}"
                    )
                    pandas_result = self.pandas_analysis(df, query)

            # 3. ì¢…í•© ì‘ë‹µ ìƒì„±
            if pandas_result:
                # Pandas ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
                combined_response = f"""ğŸ“Š **ë°ì´í„° ë¶„ì„ ê²°ê³¼:**\n{pandas_result}"""
            else:
                combined_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            return {
                "response": combined_response,
                "data_analysis_available": df is not None and not df.empty,
            }

        except Exception as e:
            logger.error(f"Error in hybrid chat: {e}")
            return {
                "response": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "data_analysis_available": False,
            }

    async def hybrid_chat_stream(
        self, query: str, df: pd.DataFrame, insurance_data: List[Dict[str, Any]]
    ):
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì˜ Hybrid RAG ì‹œìŠ¤í…œ - ë‹¨ê³„ë³„ ì§„í–‰ ìƒíƒœ ì „ì†¡
        """
        start_time = time.time()
        logger.info(f"[STREAM START] Streaming Hybrid RAG ì±— ì‹œì‘ - ì¿¼ë¦¬: '{query}'")

        # ì§„í–‰ë¥  ê°€ì¤‘ì¹˜ ì •ì˜ (preparingì„ searchingì— í†µí•©)
        PROGRESS_WEIGHTS = {
            "searching": 0.25,  # ë²¡í„° ê²€ìƒ‰ + ë°ì´í„° ì¤€ë¹„ 25%
            "analyzing": 0.60,  # Pandas ë¶„ì„ 60%
            "finalizing": 0.15,  # ìµœì¢… ì •ë¦¬ 15%
        }

        try:
            logger.info(f"[STREAM] ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ - ì¿¼ë¦¬: '{query}'")

            # 1. ë²¡í„° ê²€ìƒ‰ ë‹¨ê³„
            logger.info("[STREAM] 1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ ì‹œì‘")
            chunk1 = {
                "status": "searching",
                "message": "ğŸ” ë²¡í„° ê²€ìƒ‰ ì¤‘...",
                "progress": 0.0,
                "timestamp": time.time(),
            }
            logger.info(f"[STREAM YIELD] ì²« ë²ˆì§¸ ì²­í¬ ì „ì†¡: {chunk1}")
            yield chunk1

            # ì•½ê°„ì˜ ì§€ì—°ì„ ì£¼ì–´ ì²­í¬ê°€ ì „ì†¡ë˜ë„ë¡ í•¨
            await asyncio.sleep(0.1)

            # ë°ì´í„° ì¤€ë¹„ ë‹¨ê³„ ì‹œì‘

            # 2. ë°ì´í„° ì¤€ë¹„ ë‹¨ê³„ (searching ìƒíƒœë¡œ í†µí•©)
            logger.info("[STREAM] 2ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ì‹œì‘")
            chunk3 = {
                "status": "searching",
                "message": "ğŸ“‹ ë°ì´í„° ì¤€ë¹„ ì¤‘...",
                "progress": PROGRESS_WEIGHTS["searching"] * 100,
                "timestamp": time.time(),
            }
            logger.info(f"[STREAM YIELD] ë°ì´í„° ì¤€ë¹„ ì‹œì‘ ì²­í¬ ì „ì†¡: {chunk3}")
            yield chunk3
            await asyncio.sleep(0.1)

            pandas_result = ""
            aggregated_df = df  # ê¸°ë³¸ê°’ìœ¼ë¡œ df ì„¤ì •

            if df is not None and not df.empty:
                # ë°ì´í„°í”„ë ˆì„ ì¤€ë¹„
                from data_manager import data_manager

                try:
                    if (
                        data_manager.coverage_premiums_df is not None
                        and not data_manager.coverage_premiums_df.empty
                    ):
                        logger.info("[STREAM] ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹œì‘")
                        normalized_df = data_manager.normalize_coverage_amounts(
                            data_manager.coverage_premiums_df
                        )
                        aggregated_df = data_manager.aggregate_coverage_by_code(
                            normalized_df
                        )
                        logger.info("[STREAM] ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"[STREAM] ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹¤íŒ¨: {e}")
                    aggregated_df = df  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ df ì‚¬ìš©

            # ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ ìƒíƒœ ì „ì†¡
            chunk4 = {
                "status": "searching",
                "message": "ğŸ“‹ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ",
                "progress": PROGRESS_WEIGHTS["searching"] * 100,
                "timestamp": time.time(),
                "data_shape": (
                    aggregated_df.shape if aggregated_df is not None else None
                ),
            }
            logger.info(f"[STREAM YIELD] ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ ì²­í¬ ì „ì†¡: {chunk4}")
            yield chunk4
            await asyncio.sleep(0.1)

            # 3. Pandas ë¶„ì„ ë‹¨ê³„ (ë‹¨ìˆœí™”)
            if aggregated_df is not None and not aggregated_df.empty:
                logger.info("[STREAM] 3ë‹¨ê³„: Pandas ë¶„ì„ ì‹œì‘")
                # ë¶„ì„ ì‹œì‘ ìƒíƒœ ì „ì†¡
                chunk5 = {
                    "status": "analyzing",
                    "message": "ğŸ“Š Pandas ë¶„ì„ ì¤‘...",
                    "progress": PROGRESS_WEIGHTS["searching"] * 100,
                    "timestamp": time.time(),
                }
                logger.info(f"[STREAM YIELD] Pandas ë¶„ì„ ì‹œì‘ ì²­í¬ ì „ì†¡: {chunk5}")
                yield chunk5
                await asyncio.sleep(0.1)

                # ì‹¤ì œ Pandas ë¶„ì„ ì‹¤í–‰
                try:
                    logger.info("[STREAM] ì‹¤ì œ pandas_analysis í˜¸ì¶œ ì‹œì‘")
                    pandas_result = self.pandas_analysis(df, query, aggregated_df)
                    logger.info(
                        f"[STREAM] Pandas ë¶„ì„ ì™„ë£Œ - ê²°ê³¼ ê¸¸ì´: {len(pandas_result) if pandas_result else 0}"
                    )
                except Exception as e:
                    logger.error(f"[STREAM] Pandas ë¶„ì„ ì‹¤íŒ¨: {e}")
                    pandas_result = f"ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

                # ë¶„ì„ ì™„ë£Œ ìƒíƒœ ì „ì†¡
                chunk6 = {
                    "status": "analyzing",
                    "message": "ğŸ“Š Pandas ë¶„ì„ ì™„ë£Œ",
                    "progress": (
                        PROGRESS_WEIGHTS["searching"] + PROGRESS_WEIGHTS["analyzing"]
                    )
                    * 100,
                    "timestamp": time.time(),
                    "result_length": len(pandas_result) if pandas_result else 0,
                }
                logger.info(f"[STREAM YIELD] Pandas ë¶„ì„ ì™„ë£Œ ì²­í¬ ì „ì†¡: {chunk6}")
                yield chunk6
                await asyncio.sleep(0.1)

            # 4. ìµœì¢… ì‘ë‹µ ìƒì„±
            logger.info("[STREAM] 4ë‹¨ê³„: ìµœì¢… ì‘ë‹µ ìƒì„± ì‹œì‘")
            chunk7 = {
                "status": "finalizing",
                "message": "ğŸ¤– ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘...",
                "progress": (
                    PROGRESS_WEIGHTS["searching"]
                    + PROGRESS_WEIGHTS["analyzing"]
                    + PROGRESS_WEIGHTS["finalizing"] * 0.5
                )
                * 100,
                "timestamp": time.time(),
            }
            logger.info(f"[STREAM YIELD] ìµœì¢… ì‘ë‹µ ìƒì„± ì‹œì‘ ì²­í¬ ì „ì†¡: {chunk7}")
            yield chunk7
            await asyncio.sleep(0.1)

            # ìµœì¢… ì‘ë‹µ ìƒì„±
            logger.info("[STREAM] ìµœì¢… ì‘ë‹µ ìƒì„± ì‹œì‘")
            if pandas_result:
                combined_response = f"""ğŸ“Š **ë°ì´í„° ë¶„ì„ ê²°ê³¼:**\n{pandas_result}"""
            else:
                combined_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            total_time = time.time() - start_time
            logger.info(
                f"[STREAM COMPLETE] Streaming Chat ì™„ë£Œ - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ, ì‘ë‹µ ê¸¸ì´: {len(combined_response)}"
            )

            # ìµœì¢… ì™„ë£Œ ìƒíƒœ ì „ì†¡
            chunk8 = {
                "status": "complete",
                "message": "âœ… ë¶„ì„ ì™„ë£Œ!",
                "progress": 100.0,
                "response": combined_response,
                "data_analysis_available": df is not None and not df.empty,
                "total_time": total_time,
                "timestamp": time.time(),
            }
            logger.info(f"[STREAM YIELD] ìµœì¢… ì™„ë£Œ ì²­í¬ ì „ì†¡: {chunk8}")
            yield chunk8

        except Exception as e:
            logger.error(
                f"[STREAM ERROR] Streaming Hybrid RAG ì˜¤ë¥˜: {type(e).__name__}: {e}"
            )
            error_chunk = {
                "status": "error",
                "message": f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "progress": 100.0,
                "timestamp": time.time(),
            }
            logger.info(f"[STREAM YIELD] ì—ëŸ¬ ì²­í¬ ì „ì†¡: {error_chunk}")
            yield error_chunk


# ì „ì—­ Hybrid RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
rag_system = HybridRAGSystem()
