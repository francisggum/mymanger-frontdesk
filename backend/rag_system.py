import chromadb
import google.genai as genai
from google.genai import types
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
import logging
import os
import time
import asyncio
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

if logger.handlers:
    for handler in logger.handlers:
        logger.removeHandler(handler)

# ë¡œê¹… ë ˆë²¨ ì„¤ì • (ë” ìƒì„¸í•œ ë¡œê·¸ë¥¼ ìœ„í•´ INFOë¡œ ì„¤ì •)
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)


# LangChain imports for pandas agent (with fallback handling)
LANGCHAIN_AVAILABLE = False
ChatGoogleGenerativeAI = None
ChatOpenAI = None
create_pandas_dataframe_agent = None
ZERO_SHOT_REACT_DESCRIPTION = "zero-shot-react-description"
TOOL_CALLING_DESCRIPTION = "tool-calling"

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain_experimental.agents import create_pandas_dataframe_agent
    from langchain_openai import ChatOpenAI

    # ìµœì‹  LangChain ë²„ì „ì—ì„œ agent_types ê²½ë¡œ ë³€ê²½
    try:
        from langchain_classic.agents.agent_types import AgentType as LangChainAgentType

        ZERO_SHOT_REACT_DESCRIPTION = LangChainAgentType.ZERO_SHOT_REACT_DESCRIPTION
    except ImportError:
        # fallback: ì§ì ‘ ë¬¸ìì—´ ì •ì˜ (ìµœì‹  ë²„ì „ì—ì„œëŠ” ë¬¸ìì—´ë„ ì§€ì›)
        ZERO_SHOT_REACT_DESCRIPTION = "zero-shot-react-description"

    LANGCHAIN_AVAILABLE = True
    logger.info("LangChain pandas agent imports ì„±ê³µ")
except ImportError as e:
    logger.warning(f"LangChain imports ì‹¤íŒ¨: {e}")
    LANGCHAIN_AVAILABLE = False

    # Fallback dummy classes
    class DummyChatGoogleGenerativeAI:
        def __init__(self, *args, **kwargs):
            raise ImportError("LangChain packages not available")

    class DummyChatOpenAI:
        def __init__(self, *args, **kwargs):
            raise ImportError("LangChain packages not available")

    def dummy_create_pandas_dataframe_agent(*args, **kwargs):
        raise ImportError("LangChain packages not available")

    ChatGoogleGenerativeAI = DummyChatGoogleGenerativeAI
    ChatOpenAI = DummyChatOpenAI
    create_pandas_dataframe_agent = dummy_create_pandas_dataframe_agent


class HybridRAGSystem:
    def __init__(self, llm_provider: str = "openai"):
        self.llm_provider = llm_provider.lower()
        self.client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
        self.embedding_model = self.client.models.embed_content
        self.llm = self.client.models.generate_content
        self.vector_store = None
        self.qa_chain = None
        self._pandas_llm = None
        # í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ íŒë‹¤ìŠ¤ ë¶„ì„ ë‹¨ê³„ ì„¤ì •
        self.pandas_analysis_stages = int(os.getenv("PANDAS_ANALYSIS_STAGES", "2"))
        logger.debug(
            f"HybridRAGSystem initialized with llm_provider={self.llm_provider}, pandas_analysis_stages={os.getenv('PANDAS_ANALYSIS_STAGES')}"
        )

    def _get_pandas_llm(self):
        """LangChain pandas agentë¥¼ ìœ„í•œ LLM ì´ˆê¸°í™” (lazy loading)"""
        if not LANGCHAIN_AVAILABLE:
            logger.error("LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ - pandas agent ìƒì„± ë¶ˆê°€")
            raise ImportError("LangChain íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        if self._pandas_llm is None:
            try:
                if self.llm_provider == "gemini":
                    if ChatGoogleGenerativeAI is None:
                        raise ImportError("ChatGoogleGenerativeAI not available")

                    model = "gemini-3-flash-preview"

                    self._pandas_llm = ChatGoogleGenerativeAI(
                        model=model,
                        temperature=0,
                        google_api_key=os.getenv("GOOGLE_API_KEY"),
                        # convert_system_message_to_human=True,
                        generate_content_config=types.GenerateContentConfig(
                            thinking_config=types.ThinkingConfig(
                                thinking_level="LOW",
                            )
                        ),
                    )
                    logger.info(
                        f"LangChain ChatGoogleGenerativeAI ì´ˆê¸°í™” ì„±ê³µ - ì‚¬ìš© model: {model}"
                    )
                elif self.llm_provider == "openai":
                    model = "qwen/qwen3-235b-a22b-2507"
                    self._pandas_llm = ChatOpenAI(
                        base_url="https://openrouter.ai/api/v1",
                        # base_url="https://api.groq.com/openai/v1",
                        model=model,
                        temperature=0,
                        openai_api_key=os.getenv("OPENAI_API_KEY"),
                        default_headers={
                            "HTTP-Referer": "http://localhost:8501",
                            "X-Title": "MyManger Frontdesk",
                        },
                    )
                    logger.info(
                        f"LangChain ChatOpenAI ì´ˆê¸°í™” ì„±ê³µ - ì‚¬ìš© model: {model}"
                    )
                else:
                    raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” LLM ì œê³µì—…ì²´: {self.llm_provider}")
            except Exception as e:
                logger.error(f"LangChain {self.llm_provider} LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise
        return self._pandas_llm

    def _create_pandas_agent(self, df: pd.DataFrame):
        """LangChain pandas agent ìƒì„±"""
        try:
            if not LANGCHAIN_AVAILABLE:
                logger.error("LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ - fallback ëª¨ë“œ ì‚¬ìš©")
                return None

            if create_pandas_dataframe_agent is None:
                logger.error("create_pandas_dataframe_agent not available")
                return None

            llm = self._get_pandas_llm()

            agent = create_pandas_dataframe_agent(
                llm,
                df,
                verbose=True,
                # agent_type=ZERO_SHOT_REACT_DESCRIPTION,
                agent_type=TOOL_CALLING_DESCRIPTION,
                handle_parsing_errors=True,
                max_iterations=20,
                max_execution_time=120,
                return_intermediate_steps=True,
                allow_dangerous_code=True,
            )

            logger.info(
                f"Pandas DataFrame Agent ìƒì„± ì„±ê³µ - DataFrame shape: {df.shape}"
            )
            return agent

        except Exception as e:
            logger.error(f"Pandas Agent ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _extract_data_info(self, df: pd.DataFrame) -> Dict[str, Any]:
        """DataFrameì˜ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            return {
                "shape": df.shape,
                "columns": list(df.columns),
                "dtypes": df.dtypes.to_dict(),
                "null_counts": df.isnull().sum().to_dict(),
                "memory_usage": df.memory_usage(deep=True).sum(),
                "sample_data": df.head(3).to_dict() if len(df) > 0 else {},
            }
        except Exception as e:
            logger.error(f"ë°ì´í„° ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    def _generate_insurance_prompt(self, query: str, df_info: Dict) -> str:
        """ë³´í—˜ ë°ì´í„° ë¶„ì„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        base_prompt = f"""
ë‹¹ì‹ ì€ ë³´í—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë³´í—˜ë£Œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¶„ì„ ëª©í‘œ: {query}
ë°ì´í„° ì •ë³´:
- í˜•íƒœ: {df_info.get('shape', 'Unknown')}
- ì»¬ëŸ¼: {df_info.get('columns', [])}
- ë°ì´í„° íƒ€ì…: {df_info.get('dtypes', {})}

ë³´í—˜ ë¶„ì„ ê°€ì´ë“œ:
1. ë³´í—˜ë£Œ ë¹„êµ: ê°€ì¥ ì €ë ´í•œ ë³´í—˜ì‚¬ ìˆœìœ„ ì œì‹œ
2. ë³´ì¥ í•­ëª© ë¶„ì„: ì•”ì§„ë‹¨ë¹„, ìƒí•´ë³´ì¥ ë“± ì£¼ìš” ë³´ì¥ ë¹„êµ  
3. íŠ¹ì§• ë¶„ì„: ê° ë³´í—˜ì‚¬ì˜ ì¥ë‹¨ì  ë° ì°¨ì´ì 
4. í•©ë¦¬ì ì¸ ì¶”ì²œ: ë¹„ìš©-íš¨ê³¼ì„± ê¸°ì¤€ ì¶”ì²œ

ë¶„ì„ ì§€ì¹¨:
- ì „ì²´ ë°ì´í„° ê¸°ë°˜ í†µê³„ì  ë¶„ì„ ìˆ˜í–‰
- íŠ¹ì´ê°’(outlier) í™•ì¸ ë° ë¶„ì„
- ë³´í—˜ì‚¬ë³„ ë³´ì¥ ë‚´ìš© ìƒì„¸ ë¹„êµ
- í•œêµ­ì–´ ë³´í—˜ ìš©ì–´ ì‚¬ìš©
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ë°ì´í„° ì œê³µ
"""

        # ì§ˆë¬¸ ìœ í˜•ë³„ ì¶”ê°€ í”„ë¡¬í”„íŠ¸
        query_lower = query.lower()
        if "ì €ë ´" in query_lower or "ì‹¼" in query_lower or "ê°€ê²©" in query_lower:
            return (
                base_prompt
                + "\n\níŠ¹íˆ ë³´í—˜ë£Œ í•©ê³„ê°€ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³ , ê°€ì„±ë¹„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."
            )
        elif (
            "ë³´ì¥" in query_lower or "ë‹´ë³´" in query_lower or "ë³´ì¥ë‚´ìš©" in query_lower
        ):
            return (
                base_prompt
                + "\n\nê° ë³´ì¥ í•­ëª©ë³„ ìƒì„¸ ë¹„êµì™€ ë³´ì¥ ë‚´ìš©ì˜ ì°¨ì´ì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
            )
        elif "ì¶”ì²œ" in query_lower or "ì–´ë–¤" in query_lower:
            return (
                base_prompt
                + "\n\nê³ ê°ì˜ ì…ì¥ì—ì„œ ê°€ì¥ í•©ë¦¬ì ì¸ ì„ íƒì„ ì¶”ì²œí•˜ê³  ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            )
        elif "ë¹„êµ" in query_lower or "ì°¨ì´" in query_lower:
            return base_prompt + "\n\në³´í—˜ì‚¬ë³„ ì°¨ì´ì ì„ ëª…í™•í•˜ê²Œ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”."

        return base_prompt

    def initialize_vector_store(self, insurance_data: List[Dict[str, Any]]):
        """
        product_insur_premiums ë°ì´í„°ë¡œ ChromaDB ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        """
        try:
            if not insurance_data:
                logger.warning(
                    "No insurance data provided for vector store initialization"
                )
                return False

            # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            self.vector_store = chromadb.Client()

            # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ
            try:
                self.vector_store.delete_collection("insurance_coverage")
            except:
                pass

            self.collection = self.vector_store.create_collection(
                name="insurance_coverage", metadata={"hnsw:space": "cosine"}
            )

            # ë¬¸ì„œ ì¤€ë¹„
            documents = []
            metadatas = []
            ids = []

            for i, item in enumerate(insurance_data):
                # insur_bojang(ë³´ì¥ì„¤ëª…) í…ìŠ¤íŠ¸ ì¶”ì¶œ
                bojang_text = item.get("insur_bojang", "")
                if bojang_text:
                    documents.append(bojang_text)
                    metadatas.append(
                        {
                            "plan_id": item.get("plan_id", ""),
                            "insur_name": item.get("insur_name", ""),
                            "insur_code": item.get("insur_code", ""),
                            "premium_amount": str(item.get("premium_amount", 0)),
                        }
                    )
                    ids.append(f"doc_{i}")

            if not documents:
                logger.warning("No valid documents created from insurance data")
                return False

            # ì„ë² ë”© ìƒì„± ë° ì €ì¥
            logger.info(f"ì„ë² ë”© ìƒì„± ì‹œì‘ - ë¬¸ì„œ ìˆ˜: {len(documents)}")
            start_time = time.time()

            result = self.embedding_model(
                model="gemini-embedding-001", contents=documents
            )

            # ì„ë² ë”© ê²°ê³¼ í™•ì¸
            if not result or not hasattr(result, "embeddings") or not result.embeddings:
                logger.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: resultê°€ ë¹„ì–´ìˆìŒ")
                return False

            embeddings = []
            for i, emb in enumerate(result.embeddings):
                if emb and hasattr(emb, "values"):
                    embeddings.append(emb.values)
                else:
                    logger.warning(f"ì„ë² ë”© {i}ê°€ ë¹„ì–´ìˆìŒ")

            if len(embeddings) != len(documents):
                logger.error(
                    f"ì„ë² ë”© ìˆ˜({len(embeddings)})ì™€ ë¬¸ì„œ ìˆ˜({len(documents)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ"
                )
                return False

            logger.info(
                f"ì„ë² ë”© ìƒì„± ì™„ë£Œ - ì„ë² ë”© ìˆ˜: {len(embeddings)}, ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ"
            )

            # ChromaDBì— ì €ì¥
            logger.info("ChromaDBì— ë¬¸ì„œ ì €ì¥ ì‹œì‘")
            try:
                # ì„ë² ë”© ë³€í™˜ ì‹œë„
                self.collection.add(
                    embeddings=embeddings,  # ì›ë³¸ ì„ë² ë”© ì‚¬ìš© (ë³€í™˜ ì‹œë„ ì•ˆ í•¨)
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids,
                )
                logger.info("ChromaDB ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"ChromaDB ì €ì¥ ì‹¤íŒ¨ (ì²« ì‹œë„): {e}")

                # fallback: ê°„ë‹¨í•œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì‹œë„
                try:
                    simple_embeddings = []
                    for emb in embeddings:
                        if emb is not None:
                            if hasattr(emb, "tolist"):
                                simple_embeddings.append(emb.tolist())
                            else:
                                simple_embeddings.append(list(emb))
                        else:
                            simple_embeddings.append([])

                    self.collection.add(
                        embeddings=simple_embeddings,
                        documents=documents,
                        metadatas=metadatas,
                        ids=ids,
                    )
                    logger.info("ChromaDB ì €ì¥ ì™„ë£Œ (fallback)")
                except Exception as e2:
                    logger.error(f"ChromaDB ì €ì¥ ì‹¤íŒ¨ (fallback): {e2}")
                    return False

            logger.info(
                f"Vector store initialized successfully with {len(documents)} documents"
            )
            return True

        except Exception as e:
            logger.error(f"Error initializing vector store: {e}")
            return False

    def search_relevant_docs(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰
        """
        start_time = time.time()
        logger.info(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘ - ì¿¼ë¦¬: '{query}', ê²€ìƒ‰ ìˆ˜: {k}")

        try:
            if not hasattr(self, "collection"):
                logger.warning("Vector store not initialized")
                return []

            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            logger.info("ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹œì‘")
            embedding_start = time.time()

            query_embedding_result = self.embedding_model(
                model="gemini-embedding-001", contents=[query]
            )

            if (
                not query_embedding_result
                or not hasattr(query_embedding_result, "embeddings")
                or not query_embedding_result.embeddings
                or len(query_embedding_result.embeddings) == 0
                or not hasattr(query_embedding_result.embeddings[0], "values")
            ):
                logger.error("ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return []

            query_embedding = query_embedding_result.embeddings[0].values
            logger.info(
                f"ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ - ì†Œìš” ì‹œê°„: {time.time() - embedding_start:.2f}ì´ˆ"
            )

            # ê²€ìƒ‰
            logger.info("ë²¡í„° ê²€ìƒ‰ ì‹œì‘")
            search_start = time.time()

            # ì¿¼ë¦¬ ì„ë² ë”© ì²˜ë¦¬
            if query_embedding is None:
                logger.error("ì¿¼ë¦¬ ì„ë² ë”©ì´ None")
                return []

            # ChromaDB ì¿¼ë¦¬ - ì—¬ëŸ¬ í˜•ì‹ ì‹œë„
            try:
                # ì²« ì‹œë„: ì›ë³¸ ì„ë² ë”© ì‚¬ìš©
                results = self.collection.query(
                    query_embeddings=[query_embedding], n_results=k
                )
            except Exception as e1:
                logger.warning(f"ì²« ì¿¼ë¦¬ ì‹œë„ ì‹¤íŒ¨: {e1}")
                try:
                    # ë‘ ë²ˆì§¸ ì‹œë„: ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    import numpy as np

                    query_array = np.array(query_embedding, dtype=np.float32)
                    results = self.collection.query(
                        query_embeddings=[query_array], n_results=k
                    )
                except Exception as e2:
                    logger.error(f"ì¿¼ë¦¬ ì‹¤íŒ¨: {e2}")
                    return []

            logger.info(
                f"ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ - ì†Œìš” ì‹œê°„: {time.time() - search_start:.2f}ì´ˆ"
            )

            # ê²°ê³¼ í¬ë§·íŒ…
            docs = []
            if (
                results
                and isinstance(results, dict)
                and "documents" in results
                and results["documents"]
                and len(results["documents"]) > 0
                and results["documents"][0]
            ):

                documents_list = results["documents"][0]  # ì²« ë²ˆì§¸ ê²°ê³¼ ì„¸íŠ¸
                metadatas_list = []
                if (
                    results.get("metadatas")
                    and isinstance(results["metadatas"], list)
                    and len(results["metadatas"]) > 0
                    and results["metadatas"][0]
                ):
                    metadatas_list = results["metadatas"][0]

                for i in range(len(documents_list)):
                    doc_data = {
                        "page_content": (
                            documents_list[i] if i < len(documents_list) else ""
                        ),
                        "metadata": (
                            metadatas_list[i] if i < len(metadatas_list) else {}
                        ),
                    }
                    docs.append(doc_data)
                    content_length = len(str(doc_data["page_content"]))
                    logger.debug(
                        f"ë¬¸ì„œ {i+1}: {doc_data['metadata'].get('insur_name', 'Unknown')} - {content_length}ì"
                    )

            total_time = time.time() - start_time
            logger.info(
                f"ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ - ì°¾ì€ ë¬¸ì„œ ìˆ˜: {len(docs)}, ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ"
            )
            return docs

        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}: {str(e)}")
            return []

    def _execute_fallback_analysis(
        self, df: pd.DataFrame, query: str
    ) -> Dict[str, Any]:
        """LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œì˜ fallback ë¶„ì„"""
        start_time = time.time()

        try:
            logger.info("Fallback ë¶„ì„ ëª¨ë“œ ì‹¤í–‰ - í†µê³„ì  ë¶„ì„ ìˆ˜í–‰")

            # ê¸°ë³¸ í†µê³„ ì •ë³´ ê³„ì‚°
            df_info = self._extract_data_info(df)
            analysis_prompt = self._generate_insurance_prompt(query, df_info)

            # ë°ì´í„° í†µê³„ ë¶„ì„
            stats = {}
            try:
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0:
                    stats = df.describe().to_dict()
                    logger.info("ê¸°ìˆ  í†µê³„ ê³„ì‚° ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"ê¸°ìˆ  í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")

            # ë³´í—˜ì‚¬ë³„ ìš”ì•½ (ê°€ëŠ¥í•œ ê²½ìš°)
            company_summary = {}
            for col in df.columns:
                if any(
                    keyword in col.lower()
                    for keyword in ["ë³´í—˜ì‚¬", "company", "insurer"]
                ):
                    try:
                        company_summary[col] = df[col].value_counts().to_dict()
                    except:
                        pass

            # ë³´í—˜ë£Œ ê´€ë ¨ ë¶„ì„ (ê°€ëŠ¥í•œ ê²½ìš°)
            premium_analysis = {}
            try:
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                for col in numeric_cols:
                    if any(
                        keyword in col.lower()
                        for keyword in ["ë³´í—˜ë£Œ", "premium", "ê¸ˆì•¡", "amount"]
                    ):
                        try:
                            premium_analysis[col] = {
                                "mean": float(df[col].mean()),
                                "min": float(df[col].min()),
                                "max": float(df[col].max()),
                                "std": float(df[col].std()),
                            }
                        except:
                            pass
            except:
                pass

            # ë¶„ì„ ê²°ê³¼ êµ¬ì„±
            analysis_result = f"""
ë°ì´í„° í†µê³„ ë¶„ì„ ê²°ê³¼:

## ê¸°ë³¸ ì •ë³´
- ë°ì´í„° í˜•íƒœ: {df.shape}
- ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}

## ìˆ˜ì¹˜í˜• ë°ì´í„° ìš”ì•½
{df.describe().to_string() if df.select_dtypes(include=[np.number]).shape[1] > 0 else 'ìˆ˜ì¹˜í˜• ë°ì´í„° ì—†ìŒ'}

## ë³´í—˜ì‚¬ë³„ í˜„í™©
{chr(10).join([f'- {k}: {v}' for k, v in company_summary.items()]) if company_summary else 'ë³´í—˜ì‚¬ ì •ë³´ ì—†ìŒ'}

## ë³´í—˜ë£Œ ê´€ë ¨ í†µê³„
{chr(10).join([f'- {k}: í‰ê·  {v["mean"]:,.0f}, ìµœì†Œ {v["min"]:,.0f}, ìµœëŒ€ {v["max"]:,.0f}' for k, v in premium_analysis.items()]) if premium_analysis else 'ë³´í—˜ë£Œ ì •ë³´ ì—†ìŒ'}

## ë¶„ì„ ì œì•ˆ
ê³ ê° ì§ˆë¬¸ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì€ ì¶”ê°€ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:
1. íŠ¹ì • ë³´í—˜ì‚¬ë³„ ìƒì„¸ ë¹„êµ
2. ë³´í—˜ë£Œ ìˆ˜ì¤€ë³„ ìˆœìœ„ ë¶„ì„
3. ë³´ì¥ í•­ëª©ë³„ ì°¨ì´ì  ë¶„ì„
"""

            duration = time.time() - start_time
            logger.info(f"Fallback ë¶„ì„ ì™„ë£Œ - ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")

            return {
                "status": "success",
                "analysis": analysis_result,
                "steps": [("fallback_analysis", "í†µê³„ì  ë¶„ì„ ìˆ˜í–‰")],
                "duration": duration,
                "mode": "fallback",
            }

        except Exception as e:
            logger.error(f"Fallback ë¶„ì„ ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
            return {"status": "error", "message": str(e)}

    def _validate_agent_result(self, result: Any) -> Dict[str, Any]:
        """Agent ì‹¤í–‰ ê²°ê³¼ ìœ íš¨ì„± ê²€ì‚¬ ë° í‘œì¤€í™”"""
        try:
            # ê²°ê³¼ í˜•ì‹ ê²€ì‚¬
            if not isinstance(result, dict):
                logger.warning(f"Agent ê²°ê³¼ê°€ dict í˜•ì‹ì´ ì•„ë‹˜: {type(result)}")
                return {"status": "invalid_format", "message": "ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜"}

            # output í•„ë“œ ê²€ì‚¬
            if "output" not in result:
                logger.warning("Agent ê²°ê³¼ì— output í•„ë“œ ì—†ìŒ")
                logger.debug(f"Agent ê²°ê³¼ í‚¤: {list(result.keys())}")
                return {"status": "missing_output", "message": "Output í•„ë“œ ëˆ„ë½"}

            agent_output = result["output"]

            # output ë‚´ìš© ê²€ì‚¬
            if not agent_output or not str(agent_output).strip():
                logger.warning("Agent outputì´ ë¹„ì–´ìˆìŒ")
                return {"status": "empty_output", "message": "ë¶„ì„ ê²°ê³¼ ì—†ìŒ"}

            # Agent outputì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
            output_str = ""
            if isinstance(agent_output, list):
                # ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©ì—ì„œ text í•„ë“œ ì¶”ì¶œ
                for item in agent_output:
                    if isinstance(item, dict) and "text" in item:
                        output_str += item["text"]
                    elif isinstance(item, str):
                        output_str += item
            else:
                output_str = str(agent_output)
            if len(output_str.strip()) < 5:
                logger.warning(f"Agent outputì´ ë„ˆë¬´ ì§§ìŒ: {len(output_str)}ì")
                return {"status": "too_short", "message": "ë¶„ì„ ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìŒ"}

            # í•œêµ­ì–´ ë‚´ìš© ê²€ì‚¬
            if not any(ord(char) > 127 for char in output_str):
                logger.warning("Agent outputì— í•œêµ­ì–´ ë‚´ìš© ì—†ìŒ")
                # ì˜ì–´ë§Œ ìˆì–´ë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬ (fallback ë°©ì§€)
                logger.info("ì˜ì–´ ì‘ë‹µì´ì§€ë§Œ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬")

            # ìµœì¢… ìœ íš¨ì„± ê²€ì‚¬ í†µê³¼
            steps = result.get("intermediate_steps", [])

            return {
                "status": "success",
                "analysis": output_str,
                "steps": steps,
                "intermediate_steps_count": len(steps),
                "output_length": len(output_str),
                "result_keys": list(result.keys()),
                "validation_time": time.time(),
            }

        except Exception as e:
            logger.error(f"Agent ê²°ê³¼ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
            return {
                "status": "validation_error",
                "message": f"ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}",
            }

    def _execute_agent_analysis(self, df: pd.DataFrame, query: str) -> Dict[str, Any]:
        """LangChain agentë¥¼ í†µí•œ ë°ì´í„° ë¶„ì„ ì‹¤í–‰ - ê°œì„ ëœ ë²„ì „"""
        start_time = time.time()

        # LangChain ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ì²´í¬
        if not LANGCHAIN_AVAILABLE:
            logger.info("LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ Fallback ëª¨ë“œë¡œ ì „í™˜")
            return self._execute_fallback_analysis(df, query)

        try:
            # Agent ìƒì„±
            logger.info("LangChain Pandas Agent ìƒì„± ì‹œì‘")
            agent = self._create_pandas_agent(df)

            if agent is None:
                logger.warning("Agent ìƒì„± ì‹¤íŒ¨ - Fallback ëª¨ë“œë¡œ ì „í™˜")
                return self._execute_fallback_analysis(df, query)

            # ë³´í—˜ íŠ¹í™” í”„ë¡¬í”„íŠ¸
            df_info = self._extract_data_info(df)
            analysis_prompt = self._generate_insurance_prompt(query, df_info)

            logger.info(f"Agent ë¶„ì„ ì‹¤í–‰ ì‹œì‘ - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(analysis_prompt)}")

            # Agent ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ ì„¤ì •
            agent_start_time = time.time()
            result = agent.invoke({"input": analysis_prompt})
            agent_execution_time = time.time() - agent_start_time

            logger.info(f"Agent ì‹¤í–‰ ì™„ë£Œ - ì†Œìš” ì‹œê°„: {agent_execution_time:.2f}ì´ˆ")

            # ê²°ê³¼ ìœ íš¨ì„± ê²€ì‚¬ ë° í‘œì¤€í™”
            validated_result = self._validate_agent_result(result)

            if validated_result["status"] == "success":
                # ì„±ê³µ ë¡œê¹…
                logger.info(
                    f"Agent ë¶„ì„ ì„±ê³µ - ê²°ê³¼ ê¸¸ì´: {validated_result['output_length']}ì, "
                    f"ë‹¨ê³„ ìˆ˜: {validated_result['intermediate_steps_count']}, "
                    f"ì‹¤í–‰ ì‹œê°„: {agent_execution_time:.2f}ì´ˆ"
                )

                # ìƒì„¸ ë¶„ì„ ë‹¨ê³„ ë¡œê¹… (ì„ íƒì )
                if validated_result["intermediate_steps_count"] > 0:
                    logger.debug("=== Agent ë¶„ì„ ë‹¨ê³„ ìƒì„¸ ===")
                    for i, step in enumerate(
                        validated_result["steps"][:3]
                    ):  # ì²˜ìŒ 3ë‹¨ê³„ë§Œ ë¡œê¹…
                        logger.debug(f"ë‹¨ê³„ {i+1}: {str(step)[:100]}...")

                # í‘œì¤€í™”ëœ ê²°ê³¼ ë°˜í™˜
                return {
                    "status": "success",
                    "analysis": validated_result["analysis"],
                    "steps": validated_result["steps"],
                    "duration": time.time() - start_time,
                    "mode": "langchain_agent",
                    "execution_time": agent_execution_time,
                    "validation_info": {
                        "output_length": validated_result["output_length"],
                        "steps_count": validated_result["intermediate_steps_count"],
                    },
                }
            else:
                # ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨ - ìƒì„¸ ë¡œê·¸ì™€ í•¨ê»˜ fallback
                logger.error(
                    f"Agent ê²°ê³¼ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {validated_result['status']} - {validated_result['message']}"
                )
                logger.debug(f"Agent ì›ë³¸ ê²°ê³¼: {str(result)[:500]}...")

                # íŠ¹ì • ì‹¤íŒ¨ ìœ í˜•ë³„ ì²˜ë¦¬
                if validated_result["status"] in ["too_short", "empty_output"]:
                    logger.warning(
                        "Agentê°€ ë¶ˆì¶©ë¶„í•œ ì‘ë‹µì„ ì œê³µ - Fallback ëª¨ë“œë¡œ ì „í™˜"
                    )
                elif validated_result["status"] in ["invalid_format", "missing_output"]:
                    logger.error("Agent ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜ - Fallback ëª¨ë“œë¡œ ì „í™˜")
                else:
                    logger.warning(
                        f"ê¸°íƒ€ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {validated_result['message']}"
                    )

                return self._execute_fallback_analysis(df, query)

        except Exception as e:
            error_time = time.time() - start_time
            logger.error(
                f"Agent ì‹¤í–‰ ì¹˜ëª…ì  ì˜¤ë¥˜: {type(e).__name__}: {str(e)} - "
                f"ì†Œìš” ì‹œê°„: {error_time:.2f}ì´ˆ - Fallback ëª¨ë“œë¡œ ì „í™˜"
            )

            # ì—ëŸ¬ íƒ€ì…ë³„ ìƒì„¸ ì²˜ë¦¬
            error_str = str(e).lower()
            if "timeout" in error_str or "ì‹œê°„ ì´ˆê³¼" in error_str:
                logger.error("Agent ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼")
            elif "parsing" in error_str or "íŒŒì‹±" in error_str:
                logger.error("Agent ì¶œë ¥ íŒŒì‹± ì˜¤ë¥˜")
            elif "memory" in error_str or "ë©”ëª¨ë¦¬" in error_str:
                logger.error("Agent ì‹¤í–‰ ë©”ëª¨ë¦¬ ì˜¤ë¥˜")

            return self._execute_fallback_analysis(df, query)

    def _generate_final_analysis(
        self, agent_result: Dict, query: str, df: pd.DataFrame
    ) -> str:
        """ìµœì¢… LLMì„ í†µí•œ ì¢…í•© ë¶„ì„"""
        start_time = time.time()

        try:
            if agent_result["status"] != "success":
                return f"ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {agent_result['message']}"

            # ìµœì¢… ì¢…í•© í”„ë¡¬í”„íŠ¸
            final_prompt = f"""
ë‹¤ìŒì€ LangChain pandas agentì˜ ë³´í—˜ ë°ì´í„° ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

{agent_result['analysis']}

ê³ ê° ì§ˆë¬¸: {query}

ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë³´í—˜ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ì¢…í•©ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. **í•µì‹¬ ë¶„ì„ ë‚´ìš© ìš”ì•½**: ê°€ì¥ ì¤‘ìš”í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½
2. **ë³´í—˜ì‚¬ë³„ íŠ¹ì§• ë¹„êµ**: ê° ë³´í—˜ì‚¬ì˜ ì¥ì , ë‹¨ì , ì°¨ì´ì  ëª…í™•íˆ ë¹„êµ  
3. **ìˆ˜ì¹˜ ê¸°ë°˜ ì¶”ì²œ**: êµ¬ì²´ì ì¸ ê¸ˆì•¡ê³¼ ë°ì´í„°ë¥¼ ê·¼ê±°ë¡œ ì¶”ì²œ
4. **ì‹¤ì§ˆì ì¸ ì¡°ì–¸**: ê³ ê°ì˜ ì…ì¥ì—ì„œ ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ì´ ë  ì •ë³´ ì œê³µ

ë‹µë³€ í˜•ì‹:
- ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ ì‚¬ìš©
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ë°ì´í„° í¬í•¨  
- ë¶ˆë › í¬ì¸íŠ¸ë‚˜ ë²ˆí˜¸ë¡œ êµ¬ì¡°í™”
- ì „ë¬¸ê°€ì ì´ë©´ì„œ ì¹œì ˆí•œ í†¤
"""

            logger.info(f"ìµœì¢… ë¶„ì„ ìƒì„± ì‹œì‘ - LLM í˜¸ì¶œ")

            # ìµœì¢… LLM ì‘ë‹µ ìƒì„±
            response = self.llm(model="gemini-3-flash-preview", contents=[final_prompt])

            result_text = "ë¶„ì„ ê²°ê³¼ ìƒì„± ì‹¤íŒ¨"
            if response and hasattr(response, "text") and response.text:
                result_text = response.text

            return result_text

        except Exception as e:
            logger.error(f"ìµœì¢… ë¶„ì„ ìƒì„± ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
            return f"ìµœì¢… ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def _optimize_dataframe_memory(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„°í”„ë ˆì„ ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            original_memory = df.memory_usage(deep=True).sum()

            # ìˆ˜ì¹˜í˜• ë°ì´í„° ìµœì í™”
            for col in df.select_dtypes(include=["int64"]).columns:
                df[col] = pd.to_numeric(df[col], downcast="integer")

            for col in df.select_dtypes(include=["float64"]).columns:
                df[col] = pd.to_numeric(df[col], downcast="float")

            # ë¬¸ìì—´ ë°ì´í„° ìµœì í™”
            for col in df.select_dtypes(include=["object"]).columns:
                if df[col].nunique() / len(df) < 0.5:  # ì¹´ë””ë„ë¦¬í‹°ê°€ ë‚®ì€ ê²½ìš°
                    df[col] = df[col].astype("category")

            optimized_memory = df.memory_usage(deep=True).sum()
            memory_reduction = (
                (original_memory - optimized_memory) / original_memory * 100
            )

            logger.info(
                f"ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ - {original_memory/1024/1024:.2f}MB â†’ {optimized_memory/1024/1024:.2f}MB ({memory_reduction:.1f}% ê°ì†Œ)"
            )

            return df

        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return df

    def pandas_analysis(
        self,
        df: pd.DataFrame,
        query: str,
        comparison_table: Optional[pd.DataFrame] = None,
    ) -> str:
        """
        ê°œì„ ëœ ë³´í—˜ë£Œ ë°ì´í„° ë¶„ì„ - LangChain pandas agent í†µí•©
        ê°€ë³€ ë‹¨ê³„ êµ¬ì¡°: 1) ë°ì´í„° ì¤€ë¹„ â†’ 2) Pandas Agent ë¶„ì„ â†’ 3) ìµœì¢… LLM ì¢…í•© (ì„ íƒì )
        """
        start_time = time.time()
        logger.info(f"=== {self.pandas_analysis_stages}ë‹¨ê³„ Pandas ë¶„ì„ ì‹œì‘ ===")
        logger.info(
            f"ì¿¼ë¦¬: '{query}', DataFrame í˜•íƒœ: {df.shape if df is not None else 'None'}"
        )

        try:
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            if df is None or df.empty:
                logger.warning("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

            # 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„
            prep_start = time.time()
            logger.info("1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ì‹œì‘")

            # íŒë‹¤ìŠ¤ ì—ì´ì „íŠ¸ ë¶„ì„ì„ ìœ„í•´ aggregated_df ì‚¬ìš© (ë³´ì¥ê¸ˆì•¡ ì •ë³´ ë³´ì¡´)
            if comparison_table is not None and not comparison_table.empty:
                # comparison_tableì´ ìˆë”ë¼ë„ aggregated_dfë¥¼ ìš°ì„  ì‚¬ìš©
                logger.info(
                    "comparison_tableì´ ìˆì§€ë§Œ aggregated_dfë¡œ ë¶„ì„ ì§„í–‰ (ë³´ì¥ê¸ˆì•¡ ì •ë³´ ë³´ì¡´)"
                )
                try:
                    from data_manager import data_manager

                    if (
                        data_manager.coverage_premiums_df is not None
                        and not data_manager.coverage_premiums_df.empty
                    ):
                        # ë™ì ìœ¼ë¡œ aggregated_df ìƒì„±
                        logger.info("ë³´í—˜ë£Œ ë°ì´í„° ì •ê·œí™” ì‹œì‘")
                        normalized_df = data_manager.normalize_coverage_amounts(
                            data_manager.coverage_premiums_df
                        )

                        logger.info("ë³´í—˜ì‚¬ë³„ ë°ì´í„° ì§‘ê³„ ì‹œì‘")
                        analysis_df = data_manager.aggregate_coverage_by_code(
                            normalized_df
                        )
                        data_type = "ì§‘ê³„ëœ ë°ì´í„°í”„ë ˆì„ (aggregated_df)"
                        logger.info(
                            f"ì§‘ê³„ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ - í˜•íƒœ: {analysis_df.shape}"
                        )
                    else:
                        analysis_df = df
                        data_type = "ì›ë³¸ ë°ì´í„°"
                        logger.info("ì›ë³¸ ë°ì´í„°ë¡œ ë¶„ì„ ì§„í–‰")
                except Exception as e:
                    logger.error(f"ì§‘ê³„ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹¤íŒ¨: {e}")
                    analysis_df = df
                    data_type = "ì›ë³¸ ë°ì´í„°(ì§‘ê³„ ì‹¤íŒ¨)"
            else:
                # ë¹„êµ í‘œê°€ ì—†ëŠ” ê²½ìš° aggregated_df ìƒì„± ì‹œë„
                logger.info("ë™ì  ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹œë„")
                try:
                    from data_manager import data_manager

                    if (
                        data_manager.coverage_premiums_df is not None
                        and not data_manager.coverage_premiums_df.empty
                    ):
                        # ë™ì ìœ¼ë¡œ aggregated_df ìƒì„±
                        logger.info("ë³´í—˜ë£Œ ë°ì´í„° ì •ê·œí™” ì‹œì‘")
                        normalized_df = data_manager.normalize_coverage_amounts(
                            data_manager.coverage_premiums_df
                        )

                        logger.info("ë³´í—˜ì‚¬ë³„ ë°ì´í„° ì§‘ê³„ ì‹œì‘")
                        analysis_df = data_manager.aggregate_coverage_by_code(
                            normalized_df
                        )
                        data_type = "ë™ì  ìƒì„± ì§‘ê³„ ë°ì´í„°í”„ë ˆì„"
                        logger.info(
                            f"ë™ì  ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ - í˜•íƒœ: {analysis_df.shape}"
                        )
                    else:
                        analysis_df = df
                        data_type = "ì›ë³¸ ë°ì´í„°"
                        logger.info("ì›ë³¸ ë°ì´í„°ë¡œ ë¶„ì„ ì§„í–‰")
                except Exception as e:
                    logger.error(f"ë™ì  ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹¤íŒ¨: {e}")
                    analysis_df = df
                    data_type = "ì›ë³¸ ë°ì´í„°(ì§‘ê³„ ì‹¤íŒ¨)"

            # ë©”ëª¨ë¦¬ ìµœì í™”
            analysis_df = self._optimize_dataframe_memory(analysis_df)

            prep_time = time.time() - prep_start
            logger.info(
                f"1ë‹¨ê³„ ì™„ë£Œ: ë°ì´í„° ì¤€ë¹„ - ìœ í˜•: {data_type}, ì†Œìš” ì‹œê°„: {prep_time:.2f}ì´ˆ"
            )

            # 2ë‹¨ê³„: LangChain Pandas Agent ë¶„ì„
            logger.info("2ë‹¨ê³„: Pandas Agent ë¶„ì„ ì‹œì‘")
            agent_start = time.time()

            agent_result = self._execute_agent_analysis(analysis_df, query)

            agent_time = time.time() - agent_start
            logger.info(
                f"2ë‹¨ê³„ ì™„ë£Œ: Agent ë¶„ì„ - ìƒíƒœ: {agent_result['status']}, ì†Œìš” ì‹œê°„: {agent_time:.2f}ì´ˆ"
            )

            # 3ë‹¨ê³„: ìµœì¢… LLM ì¢…í•© ë¶„ì„ (ë‹¨ê³„ ìˆ˜ ì„¤ì •ì— ë”°ë¼ ì‹¤í–‰)

            # Agent ê²°ê³¼ ì•ˆì „ ì¶”ì¶œ - ìƒˆë¡œìš´ ìœ íš¨ì„± ê²€ì‚¬ ê²°ê³¼ êµ¬ì¡° ë°˜ì˜
            if agent_result.get("status") == "success":
                final_result = agent_result.get("analysis", "")
                # ì„±ê³µ ì‹œ ì¶”ê°€ ì •ë³´ ë¡œê¹…
                validation_info = agent_result.get("validation_info", {})
                if validation_info:
                    logger.info(
                        f"Agent ë¶„ì„ ìƒì„¸ - ê¸¸ì´: {validation_info.get('output_length', 0)}ì, "
                        f"ë‹¨ê³„: {validation_info.get('steps_count', 0)}ê°œ"
                    )
            else:
                # Agent ì‹¤íŒ¨ ì‹œ fallback ê²°ê³¼ ì‚¬ìš©
                logger.warning(
                    f"Agent ë¶„ì„ ì‹¤íŒ¨: {agent_result.get('status', 'unknown')}"
                )
                final_result = agent_result.get(
                    "analysis", "ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )

            if self.pandas_analysis_stages >= 3:
                logger.info("3ë‹¨ê³„: ìµœì¢… LLM ì¢…í•© ë¶„ì„ ì‹œì‘")
                final_start = time.time()

                final_result = self._generate_final_analysis(
                    agent_result, query, analysis_df
                )

                final_time = time.time() - final_start
                logger.info(f"3ë‹¨ê³„ ì™„ë£Œ: ìµœì¢… ë¶„ì„ - ì†Œìš” ì‹œê°„: {final_time:.2f}ì´ˆ")
            else:
                logger.info(
                    f"3ë‹¨ê³„ ê±´ë„ˆëœ€ - ì„¤ì •ëœ ë‹¨ê³„ ìˆ˜: {self.pandas_analysis_stages}"
                )

            total_time = time.time() - start_time
            logger.info(
                f"=== ì „ì²´ ë¶„ì„ ì™„ë£Œ ({self.pandas_analysis_stages}ë‹¨ê³„) === ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ, ê²°ê³¼ ê¸¸ì´: {len(final_result)}ì"
            )

            return final_result

        except Exception as e:
            error_time = time.time() - start_time
            logger.error(f"pandas_analysis ì¹˜ëª…ì  ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ ì‹œì : {error_time:.2f}ì´ˆ")
            logger.error(f"ë°ì´í„° ì •ë³´: shape={df.shape if df is not None else 'None'}")

            # ì‚¬ìš©ì ì¹œí™”ì  ì—ëŸ¬ ë©”ì‹œì§€
            error_message = self._generate_user_friendly_error(e)
            return error_message

    def _generate_user_friendly_error(self, error: Exception) -> str:
        """ì‚¬ìš©ì ì¹œí™”ì  ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±"""
        error_str = str(error).lower()
        error_type = type(error).__name__

        if "timeout" in error_str or "ì‹œê°„" in error_str:
            return "â° ë¶„ì„ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°ê°€ ë„ˆë¬´ ë§ê±°ë‚˜ ë³µì¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "memory" in error_str or "ë©”ëª¨ë¦¬" in error_str:
            return "ğŸ’¾ ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì•„ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¼ë¶€ ë°ì´í„°ë§Œ ë‹¤ì‹œ ë¶„ì„í•´ì£¼ì„¸ìš”."
        elif "api" in error_str or "ì—°ê²°" in error_str:
            return "ğŸ”Œ ì™¸ë¶€ API ì—°ê²°ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "parse" in error_str or "í˜•ì‹" in error_str:
            return (
                "ğŸ“‹ ë°ì´í„° í˜•ì‹ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            )
        else:
            return f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(error)} (ì˜¤ë¥˜ íƒ€ì…: {error_type})"

    def hybrid_chat(
        self, query: str, df: pd.DataFrame, insurance_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Hybrid RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ ì¢…í•©ì ì¸ ì§ˆì˜ì‘ë‹µ - ë¹„êµ í‘œ í™œìš©
        """
        start_time = time.time()
        logger.info(f"Hybrid RAG ì±— ì‹œì‘ - ì¿¼ë¦¬: '{query}'")
        logger.info(
            f"ì…ë ¥ ë°ì´í„° - DataFrame í˜•íƒœ: {df.shape if df is not None else 'None'}, ë³´í—˜ ë°ì´í„° ìˆ˜: {len(insurance_data) if insurance_data else 0}"
        )

        try:
            # 1. ë²¡í„° ê²€ìƒ‰ì„ í†µí•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            logger.info("1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ ì‹œì‘")
            search_start = time.time()
            relevant_docs = self.search_relevant_docs(query)
            search_time = time.time() - search_start
            logger.info(
                f"1ë‹¨ê³„ ì™„ë£Œ: ë²¡í„° ê²€ìƒ‰ - ì°¾ì€ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}, ì†Œìš” ì‹œê°„: {search_time:.2f}ì´ˆ"
            )

            # 2. ë¹„êµ í‘œ ìƒì„± ë° Pandas ë°ì´í„° ë¶„ì„
            pandas_result = ""
            comparison_table = None

            if df is not None and not df.empty:
                # ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹œë„
                from data_manager import data_manager

                try:
                    if (
                        data_manager.coverage_premiums_df is not None
                        and not data_manager.coverage_premiums_df.empty
                    ):
                        # ë™ì ìœ¼ë¡œ ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± (ë³´ì¥ê¸ˆì•¡ ì •ë³´ ë³´ì¡´)
                        logger.info("í•˜ì´ë¸Œë¦¬ë“œ ì±—ì—ì„œ ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹œì‘")
                        normalized_df = data_manager.normalize_coverage_amounts(
                            data_manager.coverage_premiums_df
                        )
                        aggregated_df = data_manager.aggregate_coverage_by_code(
                            normalized_df
                        )
                        logger.info(
                            "ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ - pandas_analysis í˜¸ì¶œ"
                        )

                        # ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ì„ ì‚¬ìš©í•œ ë¶„ì„
                        pandas_result = self.pandas_analysis(df, query, aggregated_df)
                    else:
                        pandas_result = self.pandas_analysis(df, query)
                except Exception as e:
                    logger.warning(
                        f"Failed to create aggregated dataframe for analysis: {e}"
                    )
                    pandas_result = self.pandas_analysis(df, query)

            # 3. ì¢…í•© ì‘ë‹µ ìƒì„±
            if relevant_docs and pandas_result:
                # ë‘ ê°€ì§€ ê²°ê³¼ ëª¨ë‘ ìˆëŠ” ê²½ìš°
                # Simple document-based QA using LLM directly
                context = "\n".join([doc["page_content"] for doc in relevant_docs])
                prompt = f"""Based on the following context, please answer the question: {query}

Context:
{context}

Answer:"""
                qa_result = self.llm(
                    model="gemini-3-flash-preview", contents=[prompt]
                ).text
                combined_response = f"""ğŸ“Š **ë°ì´í„° ë¶„ì„ ê²°ê³¼:**\n{pandas_result}\n\nğŸ“‹ **ë³´ì¥ë‚´ìš© ê²€ìƒ‰ ê²°ê³¼:**\n{qa_result}"""

            elif relevant_docs:
                # ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ë§Œ ìˆëŠ” ê²½ìš°
                context = "\n".join([doc["page_content"] for doc in relevant_docs])
                prompt = f"""Based on the following context, please answer the question: {query}

Context:
{context}

Answer:"""
                qa_result = self.llm(
                    model="gemini-3-flash-preview", contents=[prompt]
                ).text
                combined_response = f"""ğŸ“‹ **ë³´ì¥ë‚´ìš© ê²€ìƒ‰ ê²°ê³¼:**\n{qa_result}"""

            elif pandas_result:
                # Pandas ë¶„ì„ ê²°ê³¼ë§Œ ìˆëŠ” ê²½ìš°
                combined_response = f"""ğŸ“Š **ë°ì´í„° ë¶„ì„ ê²°ê³¼:**\n{pandas_result}"""

            else:
                combined_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            return {
                "response": combined_response,
                "sources_found": len(relevant_docs) > 0,
                "data_analysis_available": df is not None and not df.empty,
                "source_count": len(relevant_docs),
            }

        except Exception as e:
            logger.error(f"Error in hybrid chat: {e}")
            return {
                "response": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources_found": False,
                "data_analysis_available": False,
                "source_count": 0,
            }

    async def hybrid_chat_stream(
        self, query: str, df: pd.DataFrame, insurance_data: List[Dict[str, Any]]
    ):
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì˜ Hybrid RAG ì‹œìŠ¤í…œ - ë‹¨ê³„ë³„ ì§„í–‰ ìƒíƒœ ì „ì†¡
        """
        start_time = time.time()
        logger.info(f"[STREAM START] Streaming Hybrid RAG ì±— ì‹œì‘ - ì¿¼ë¦¬: '{query}'")

        # ì§„í–‰ë¥  ê°€ì¤‘ì¹˜ ì •ì˜ (preparingì„ searchingì— í†µí•©)
        PROGRESS_WEIGHTS = {
            "searching": 0.25,  # ë²¡í„° ê²€ìƒ‰ + ë°ì´í„° ì¤€ë¹„ 25%
            "analyzing": 0.60,  # Pandas ë¶„ì„ 60%
            "finalizing": 0.15,  # ìµœì¢… ì •ë¦¬ 15%
        }

        try:
            logger.info(f"[STREAM] ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ - ì¿¼ë¦¬: '{query}'")

            # 1. ë²¡í„° ê²€ìƒ‰ ë‹¨ê³„
            logger.info("[STREAM] 1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ ì‹œì‘")
            chunk1 = {
                "status": "searching",
                "message": "ğŸ” ë²¡í„° ê²€ìƒ‰ ì¤‘...",
                "progress": 0.0,
                "timestamp": time.time(),
            }
            logger.info(f"[STREAM YIELD] ì²« ë²ˆì§¸ ì²­í¬ ì „ì†¡: {chunk1}")
            yield chunk1

            # ì•½ê°„ì˜ ì§€ì—°ì„ ì£¼ì–´ ì²­í¬ê°€ ì „ì†¡ë˜ë„ë¡ í•¨
            await asyncio.sleep(0.1)

            search_start = time.time()
            relevant_docs = self.search_relevant_docs(query)
            search_time = time.time() - search_start

            logger.info(
                f"[STREAM] ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ - ì°¾ì€ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}, ì†Œìš” ì‹œê°„: {search_time:.2f}ì´ˆ"
            )

            # ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ ìƒíƒœ ì „ì†¡
            chunk2 = {
                "status": "searching",
                "message": f"ğŸ” ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ ({len(relevant_docs)}ê°œ ë¬¸ì„œ ë°œê²¬)",
                "progress": PROGRESS_WEIGHTS["searching"] * 100,
                "timestamp": time.time(),
                "doc_count": len(relevant_docs),
            }
            logger.info(f"[STREAM YIELD] ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ ì²­í¬ ì „ì†¡: {chunk2}")
            yield chunk2
            await asyncio.sleep(0.1)

            # 2. ë°ì´í„° ì¤€ë¹„ ë‹¨ê³„ (searching ìƒíƒœë¡œ í†µí•©)
            logger.info("[STREAM] 2ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ì‹œì‘")
            chunk3 = {
                "status": "searching",
                "message": "ğŸ“‹ ë°ì´í„° ì¤€ë¹„ ì¤‘...",
                "progress": PROGRESS_WEIGHTS["searching"] * 100,
                "timestamp": time.time(),
            }
            logger.info(f"[STREAM YIELD] ë°ì´í„° ì¤€ë¹„ ì‹œì‘ ì²­í¬ ì „ì†¡: {chunk3}")
            yield chunk3
            await asyncio.sleep(0.1)

            pandas_result = ""
            aggregated_df = df  # ê¸°ë³¸ê°’ìœ¼ë¡œ df ì„¤ì •

            if df is not None and not df.empty:
                # ë°ì´í„°í”„ë ˆì„ ì¤€ë¹„
                from data_manager import data_manager

                try:
                    if (
                        data_manager.coverage_premiums_df is not None
                        and not data_manager.coverage_premiums_df.empty
                    ):
                        logger.info("[STREAM] ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹œì‘")
                        normalized_df = data_manager.normalize_coverage_amounts(
                            data_manager.coverage_premiums_df
                        )
                        aggregated_df = data_manager.aggregate_coverage_by_code(
                            normalized_df
                        )
                        logger.info("[STREAM] ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"[STREAM] ì§‘ê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹¤íŒ¨: {e}")
                    aggregated_df = df  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ df ì‚¬ìš©

            # ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ ìƒíƒœ ì „ì†¡
            chunk4 = {
                "status": "searching",
                "message": "ğŸ“‹ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ",
                "progress": PROGRESS_WEIGHTS["searching"] * 100,
                "timestamp": time.time(),
                "data_shape": (
                    aggregated_df.shape if aggregated_df is not None else None
                ),
            }
            logger.info(f"[STREAM YIELD] ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ ì²­í¬ ì „ì†¡: {chunk4}")
            yield chunk4
            await asyncio.sleep(0.1)

            # 3. Pandas ë¶„ì„ ë‹¨ê³„ (ë‹¨ìˆœí™”)
            if aggregated_df is not None and not aggregated_df.empty:
                logger.info("[STREAM] 3ë‹¨ê³„: Pandas ë¶„ì„ ì‹œì‘")
                # ë¶„ì„ ì‹œì‘ ìƒíƒœ ì „ì†¡
                chunk5 = {
                    "status": "analyzing",
                    "message": "ğŸ“Š Pandas ë¶„ì„ ì¤‘...",
                    "progress": PROGRESS_WEIGHTS["searching"] * 100,
                    "timestamp": time.time(),
                }
                logger.info(f"[STREAM YIELD] Pandas ë¶„ì„ ì‹œì‘ ì²­í¬ ì „ì†¡: {chunk5}")
                yield chunk5
                await asyncio.sleep(0.1)

                # ì‹¤ì œ Pandas ë¶„ì„ ì‹¤í–‰
                try:
                    logger.info("[STREAM] ì‹¤ì œ pandas_analysis í˜¸ì¶œ ì‹œì‘")
                    pandas_result = self.pandas_analysis(df, query, aggregated_df)
                    logger.info(
                        f"[STREAM] Pandas ë¶„ì„ ì™„ë£Œ - ê²°ê³¼ ê¸¸ì´: {len(pandas_result) if pandas_result else 0}"
                    )
                except Exception as e:
                    logger.error(f"[STREAM] Pandas ë¶„ì„ ì‹¤íŒ¨: {e}")
                    pandas_result = f"ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

                # ë¶„ì„ ì™„ë£Œ ìƒíƒœ ì „ì†¡
                chunk6 = {
                    "status": "analyzing",
                    "message": "ğŸ“Š Pandas ë¶„ì„ ì™„ë£Œ",
                    "progress": (
                        PROGRESS_WEIGHTS["searching"] + PROGRESS_WEIGHTS["analyzing"]
                    )
                    * 100,
                    "timestamp": time.time(),
                    "result_length": len(pandas_result) if pandas_result else 0,
                }
                logger.info(f"[STREAM YIELD] Pandas ë¶„ì„ ì™„ë£Œ ì²­í¬ ì „ì†¡: {chunk6}")
                yield chunk6
                await asyncio.sleep(0.1)

            # 4. ìµœì¢… ì‘ë‹µ ìƒì„±
            logger.info("[STREAM] 4ë‹¨ê³„: ìµœì¢… ì‘ë‹µ ìƒì„± ì‹œì‘")
            chunk7 = {
                "status": "finalizing",
                "message": "ğŸ¤– ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘...",
                "progress": (
                    PROGRESS_WEIGHTS["searching"]
                    + PROGRESS_WEIGHTS["analyzing"]
                    + PROGRESS_WEIGHTS["finalizing"] * 0.5
                )
                * 100,
                "timestamp": time.time(),
            }
            logger.info(f"[STREAM YIELD] ìµœì¢… ì‘ë‹µ ìƒì„± ì‹œì‘ ì²­í¬ ì „ì†¡: {chunk7}")
            yield chunk7
            await asyncio.sleep(0.1)

            # ìµœì¢… ì‘ë‹µ ìƒì„±
            logger.info("[STREAM] ìµœì¢… LLM ì‘ë‹µ ìƒì„± ì‹œì‘")
            if relevant_docs and pandas_result:
                context = "\n".join([doc["page_content"] for doc in relevant_docs])
                prompt = f"""Based on the following context, please answer the question: {query}

Context:
{context}

Answer:"""
                qa_result = self.llm(
                    model="gemini-3-pro-preview", contents=[prompt]
                ).text
                combined_response = f"""ğŸ“Š **ë°ì´í„° ë¶„ì„ ê²°ê³¼:**\n{pandas_result}\n\nğŸ“‹ **ë³´ì¥ë‚´ìš© ê²€ìƒ‰ ê²°ê³¼:**\n{qa_result}"""
            elif relevant_docs:
                context = "\n".join([doc["page_content"] for doc in relevant_docs])
                prompt = f"""Based on the following context, please answer the question: {query}

Context:
{context}

Answer:"""
                qa_result = self.llm(
                    model="gemini-3-pro-preview", contents=[prompt]
                ).text
                combined_response = f"""ğŸ“‹ **ë³´ì¥ë‚´ìš© ê²€ìƒ‰ ê²°ê³¼:**\n{qa_result}"""
            elif pandas_result:
                combined_response = f"""ğŸ“Š **ë°ì´í„° ë¶„ì„ ê²°ê³¼:**\n{pandas_result}"""
            else:
                combined_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            total_time = time.time() - start_time
            logger.info(
                f"[STREAM COMPLETE] Streaming Hybrid RAG ì™„ë£Œ - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ, ì‘ë‹µ ê¸¸ì´: {len(combined_response)}"
            )

            # ìµœì¢… ì™„ë£Œ ìƒíƒœ ì „ì†¡
            chunk8 = {
                "status": "complete",
                "message": "âœ… ë¶„ì„ ì™„ë£Œ!",
                "progress": 100.0,
                "response": combined_response,
                "sources_found": len(relevant_docs) > 0,
                "data_analysis_available": df is not None and not df.empty,
                "source_count": len(relevant_docs),
                "total_time": total_time,
                "timestamp": time.time(),
            }
            logger.info(f"[STREAM YIELD] ìµœì¢… ì™„ë£Œ ì²­í¬ ì „ì†¡: {chunk8}")
            yield chunk8

        except Exception as e:
            logger.error(
                f"[STREAM ERROR] Streaming Hybrid RAG ì˜¤ë¥˜: {type(e).__name__}: {e}"
            )
            error_chunk = {
                "status": "error",
                "message": f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "progress": 100.0,
                "timestamp": time.time(),
            }
            logger.info(f"[STREAM YIELD] ì—ëŸ¬ ì²­í¬ ì „ì†¡: {error_chunk}")
            yield error_chunk


# ì „ì—­ Hybrid RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
rag_system = HybridRAGSystem()
