import chromadb
import google.genai as genai
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
import logging
import os
import time
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

# ë¡œê¹… ë ˆë²¨ ì„¤ì • (ë” ìƒì„¸í•œ ë¡œê·¸ë¥¼ ìœ„í•´ INFOë¡œ ì„¤ì •)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# LangChain imports for pandas agent (with fallback handling)
LANGCHAIN_AVAILABLE = False
ChatGoogleGenerativeAI = None
create_pandas_dataframe_agent = None
ZERO_SHOT_REACT_DESCRIPTION = "zero-shot-react-description"

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain_experimental.agents import create_pandas_dataframe_agent
    # ìµœì‹  LangChain ë²„ì „ì—ì„œ agent_types ê²½ë¡œ ë³€ê²½
    try:
        from langchain_classic.agents.agent_types import AgentType as LangChainAgentType
        ZERO_SHOT_REACT_DESCRIPTION = LangChainAgentType.ZERO_SHOT_REACT_DESCRIPTION
    except ImportError:
        # fallback: ì§ì ‘ ë¬¸ìì—´ ì •ì˜ (ìµœì‹  ë²„ì „ì—ì„œëŠ” ë¬¸ìì—´ë„ ì§€ì›)
        ZERO_SHOT_REACT_DESCRIPTION = "zero-shot-react-description"
    
    LANGCHAIN_AVAILABLE = True
    logger.info("LangChain pandas agent imports ì„±ê³µ")
except ImportError as e:
    logger.warning(f"LangChain imports ì‹¤íŒ¨: {e}")
    LANGCHAIN_AVAILABLE = False
    
    # Fallback dummy classes
    class DummyChatGoogleGenerativeAI:
        def __init__(self, *args, **kwargs):
            raise ImportError("LangChain packages not available")
    
    def dummy_create_pandas_dataframe_agent(*args, **kwargs):
        raise ImportError("LangChain packages not available")
    
    ChatGoogleGenerativeAI = DummyChatGoogleGenerativeAI
    create_pandas_dataframe_agent = dummy_create_pandas_dataframe_agent

class HybridRAGSystem:
    def __init__(self):
        self.client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
        self.embedding_model = self.client.models.embed_content
        self.llm = self.client.models.generate_content
        self.vector_store = None
        self.qa_chain = None
        self._pandas_llm = None
        
    def _get_pandas_llm(self):
        """LangChain pandas agentë¥¼ ìœ„í•œ LLM ì´ˆê¸°í™” (lazy loading)"""
        if not LANGCHAIN_AVAILABLE:
            logger.error("LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ - pandas agent ìƒì„± ë¶ˆê°€")
            raise ImportError("LangChain íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
        if self._pandas_llm is None:
            try:
                if ChatGoogleGenerativeAI is None:
                    raise ImportError("ChatGoogleGenerativeAI not available")
                    
                self._pandas_llm = ChatGoogleGenerativeAI(
                    model="gemini-3-pro-preview",
                    temperature=0.1,
                    google_api_key=os.getenv("GOOGLE_API_KEY"),
                    convert_system_message_to_human=True
                )
                logger.info("LangChain ChatGoogleGenerativeAI ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                logger.error(f"LangChain ChatGoogleGenerativeAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise
        return self._pandas_llm
    
    def _create_pandas_agent(self, df: pd.DataFrame):
        """LangChain pandas agent ìƒì„±"""
        try:
            if not LANGCHAIN_AVAILABLE:
                logger.error("LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ - fallback ëª¨ë“œ ì‚¬ìš©")
                return None
                
            if create_pandas_dataframe_agent is None:
                logger.error("create_pandas_dataframe_agent not available")
                return None
                
            llm = self._get_pandas_llm()
            
            agent = create_pandas_dataframe_agent(
                llm,
                df,
                verbose=True,
                agent_type=ZERO_SHOT_REACT_DESCRIPTION,
                handle_parsing_errors=True,
                max_iterations=5,
                return_intermediate_steps=True,
                allow_dangerous_code=True
            )
            
            logger.info(f"Pandas DataFrame Agent ìƒì„± ì„±ê³µ - DataFrame shape: {df.shape}")
            return agent
            
        except Exception as e:
            logger.error(f"Pandas Agent ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_data_info(self, df: pd.DataFrame) -> Dict[str, Any]:
        """DataFrameì˜ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            return {
                "shape": df.shape,
                "columns": list(df.columns),
                "dtypes": df.dtypes.to_dict(),
                "null_counts": df.isnull().sum().to_dict(),
                "memory_usage": df.memory_usage(deep=True).sum(),
                "sample_data": df.head(3).to_dict() if len(df) > 0 else {}
            }
        except Exception as e:
            logger.error(f"ë°ì´í„° ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _generate_insurance_prompt(self, query: str, df_info: Dict) -> str:
        """ë³´í—˜ ë°ì´í„° ë¶„ì„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        base_prompt = f"""
ë‹¹ì‹ ì€ ë³´í—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë³´í—˜ë£Œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¶„ì„ ëª©í‘œ: {query}
ë°ì´í„° ì •ë³´:
- í˜•íƒœ: {df_info.get('shape', 'Unknown')}
- ì»¬ëŸ¼: {df_info.get('columns', [])}
- ë°ì´í„° íƒ€ì…: {df_info.get('dtypes', {})}

ë³´í—˜ ë¶„ì„ ê°€ì´ë“œ:
1. ë³´í—˜ë£Œ ë¹„êµ: ê°€ì¥ ì €ë ´í•œ ë³´í—˜ì‚¬ ìˆœìœ„ ì œì‹œ
2. ë³´ì¥ í•­ëª© ë¶„ì„: ì•”ì§„ë‹¨ë¹„, ìƒí•´ë³´ì¥ ë“± ì£¼ìš” ë³´ì¥ ë¹„êµ  
3. íŠ¹ì§• ë¶„ì„: ê° ë³´í—˜ì‚¬ì˜ ì¥ë‹¨ì  ë° ì°¨ì´ì 
4. í•©ë¦¬ì ì¸ ì¶”ì²œ: ë¹„ìš©-íš¨ê³¼ì„± ê¸°ì¤€ ì¶”ì²œ

ë¶„ì„ ì§€ì¹¨:
- ì „ì²´ ë°ì´í„° ê¸°ë°˜ í†µê³„ì  ë¶„ì„ ìˆ˜í–‰
- íŠ¹ì´ê°’(outlier) í™•ì¸ ë° ë¶„ì„
- ë³´í—˜ì‚¬ë³„ ë³´ì¥ ë‚´ìš© ìƒì„¸ ë¹„êµ
- í•œêµ­ì–´ ë³´í—˜ ìš©ì–´ ì‚¬ìš©
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ë°ì´í„° ì œê³µ
"""
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ì¶”ê°€ í”„ë¡¬í”„íŠ¸
        query_lower = query.lower()
        if "ì €ë ´" in query_lower or "ì‹¼" in query_lower or "ê°€ê²©" in query_lower:
            return base_prompt + "\n\níŠ¹íˆ ë³´í—˜ë£Œ í•©ê³„ê°€ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³ , ê°€ì„±ë¹„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."
        elif "ë³´ì¥" in query_lower or "ë‹´ë³´" in query_lower or "ë³´ì¥ë‚´ìš©" in query_lower:
            return base_prompt + "\n\nê° ë³´ì¥ í•­ëª©ë³„ ìƒì„¸ ë¹„êµì™€ ë³´ì¥ ë‚´ìš©ì˜ ì°¨ì´ì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
        elif "ì¶”ì²œ" in query_lower or "ì–´ë–¤" in query_lower:
            return base_prompt + "\n\nê³ ê°ì˜ ì…ì¥ì—ì„œ ê°€ì¥ í•©ë¦¬ì ì¸ ì„ íƒì„ ì¶”ì²œí•˜ê³  ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        elif "ë¹„êµ" in query_lower or "ì°¨ì´" in query_lower:
            return base_prompt + "\n\në³´í—˜ì‚¬ë³„ ì°¨ì´ì ì„ ëª…í™•í•˜ê²Œ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”."
        
        return base_prompt

        
    def initialize_vector_store(self, insurance_data: List[Dict[str, Any]]):
        """
        product_insur_premiums ë°ì´í„°ë¡œ ChromaDB ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        """
        try:
            if not insurance_data:
                logger.warning("No insurance data provided for vector store initialization")
                return False
            
            # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            self.vector_store = chromadb.Client()
            
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ
            try:
                self.vector_store.delete_collection("insurance_coverage")
            except:
                pass
                
            self.collection = self.vector_store.create_collection(
                name="insurance_coverage",
                metadata={"hnsw:space": "cosine"}
            )
            
            # ë¬¸ì„œ ì¤€ë¹„
            documents = []
            metadatas = []
            ids = []
            
            for i, item in enumerate(insurance_data):
                # insur_bojang(ë³´ì¥ì„¤ëª…) í…ìŠ¤íŠ¸ ì¶”ì¶œ
                bojang_text = item.get("insur_bojang", "")
                if bojang_text:
                    documents.append(bojang_text)
                    metadatas.append({
                        "plan_id": item.get("plan_id", ""),
                        "insur_name": item.get("insur_name", ""),
                        "insur_code": item.get("insur_code", ""),
                        "premium_amount": str(item.get("premium_amount", 0))
                    })
                    ids.append(f"doc_{i}")
            
            if not documents:
                logger.warning("No valid documents created from insurance data")
                return False
            
            # ì„ë² ë”© ìƒì„± ë° ì €ì¥
            logger.info(f"ì„ë² ë”© ìƒì„± ì‹œì‘ - ë¬¸ì„œ ìˆ˜: {len(documents)}")
            start_time = time.time()
            
            result = self.embedding_model(
                model="gemini-embedding-001",
                contents=documents
            )
            
            # ì„ë² ë”© ê²°ê³¼ í™•ì¸
            if not result or not hasattr(result, 'embeddings') or not result.embeddings:
                logger.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: resultê°€ ë¹„ì–´ìˆìŒ")
                return False
                
            embeddings = []
            for i, emb in enumerate(result.embeddings):
                if emb and hasattr(emb, 'values'):
                    embeddings.append(emb.values)
                else:
                    logger.warning(f"ì„ë² ë”© {i}ê°€ ë¹„ì–´ìˆìŒ")
            
            if len(embeddings) != len(documents):
                logger.error(f"ì„ë² ë”© ìˆ˜({len(embeddings)})ì™€ ë¬¸ì„œ ìˆ˜({len(documents)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ")
                return False
            
            logger.info(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ - ì„ë² ë”© ìˆ˜: {len(embeddings)}, ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
            
            # ChromaDBì— ì €ì¥
            logger.info("ChromaDBì— ë¬¸ì„œ ì €ì¥ ì‹œì‘")
            try:
                # ì„ë² ë”© ë³€í™˜ ì‹œë„
                self.collection.add(
                    embeddings=embeddings,  # ì›ë³¸ ì„ë² ë”© ì‚¬ìš© (ë³€í™˜ ì‹œë„ ì•ˆ í•¨)
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )
                logger.info("ChromaDB ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"ChromaDB ì €ì¥ ì‹¤íŒ¨ (ì²« ì‹œë„): {e}")
                
                # fallback: ê°„ë‹¨í•œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì‹œë„
                try:
                    simple_embeddings = []
                    for emb in embeddings:
                        if emb is not None:
                            if hasattr(emb, 'tolist'):
                                simple_embeddings.append(emb.tolist())
                            else:
                                simple_embeddings.append(list(emb))
                        else:
                            simple_embeddings.append([])
                    
                    self.collection.add(
                        embeddings=simple_embeddings,
                        documents=documents,
                        metadatas=metadatas,
                        ids=ids
                    )
                    logger.info("ChromaDB ì €ì¥ ì™„ë£Œ (fallback)")
                except Exception as e2:
                    logger.error(f"ChromaDB ì €ì¥ ì‹¤íŒ¨ (fallback): {e2}")
                    return False
            
            logger.info(f"Vector store initialized successfully with {len(documents)} documents")
            return True
            
        except Exception as e:
            logger.error(f"Error initializing vector store: {e}")
            return False
    
    def search_relevant_docs(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰
        """
        start_time = time.time()
        logger.info(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘ - ì¿¼ë¦¬: '{query}', ê²€ìƒ‰ ìˆ˜: {k}")
        
        try:
            if not hasattr(self, 'collection'):
                logger.warning("Vector store not initialized")
                return []
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            logger.info("ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹œì‘")
            embedding_start = time.time()
            
            query_embedding_result = self.embedding_model(
                model="gemini-embedding-001",
                contents=[query]
            )
            
            if (not query_embedding_result or 
                not hasattr(query_embedding_result, 'embeddings') or 
                not query_embedding_result.embeddings or
                len(query_embedding_result.embeddings) == 0 or
                not hasattr(query_embedding_result.embeddings[0], 'values')):
                logger.error("ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return []
            
            query_embedding = query_embedding_result.embeddings[0].values
            logger.info(f"ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ - ì†Œìš” ì‹œê°„: {time.time() - embedding_start:.2f}ì´ˆ")
            
            # ê²€ìƒ‰
            logger.info("ë²¡í„° ê²€ìƒ‰ ì‹œì‘")
            search_start = time.time()
            
            # ì¿¼ë¦¬ ì„ë² ë”© ì²˜ë¦¬
            if query_embedding is None:
                logger.error("ì¿¼ë¦¬ ì„ë² ë”©ì´ None")
                return []
            
            # ChromaDB ì¿¼ë¦¬ - ì—¬ëŸ¬ í˜•ì‹ ì‹œë„
            try:
                # ì²« ì‹œë„: ì›ë³¸ ì„ë² ë”© ì‚¬ìš©
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=k
                )
            except Exception as e1:
                logger.warning(f"ì²« ì¿¼ë¦¬ ì‹œë„ ì‹¤íŒ¨: {e1}")
                try:
                    # ë‘ ë²ˆì§¸ ì‹œë„: ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    import numpy as np
                    query_array = np.array(query_embedding, dtype=np.float32)
                    results = self.collection.query(
                        query_embeddings=[query_array],
                        n_results=k
                    )
                except Exception as e2:
                    logger.error(f"ì¿¼ë¦¬ ì‹¤íŒ¨: {e2}")
                    return []
            
            logger.info(f"ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ - ì†Œìš” ì‹œê°„: {time.time() - search_start:.2f}ì´ˆ")
            
            # ê²°ê³¼ í¬ë§·íŒ…
            docs = []
            if (results and 
                isinstance(results, dict) and 
                'documents' in results and 
                results['documents'] and 
                len(results['documents']) > 0 and
                results['documents'][0]):
                
                documents_list = results['documents'][0]  # ì²« ë²ˆì§¸ ê²°ê³¼ ì„¸íŠ¸
                metadatas_list = []
                if (results.get('metadatas') and 
                    isinstance(results['metadatas'], list) and 
                    len(results['metadatas']) > 0 and
                    results['metadatas'][0]):
                    metadatas_list = results['metadatas'][0]
                
                for i in range(len(documents_list)):
                    doc_data = {
                        'page_content': documents_list[i] if i < len(documents_list) else '',
                        'metadata': metadatas_list[i] if i < len(metadatas_list) else {}
                    }
                    docs.append(doc_data)
                    content_length = len(str(doc_data['page_content']))
                    logger.debug(f"ë¬¸ì„œ {i+1}: {doc_data['metadata'].get('insur_name', 'Unknown')} - {content_length}ì")
            
            total_time = time.time() - start_time
            logger.info(f"ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ - ì°¾ì€ ë¬¸ì„œ ìˆ˜: {len(docs)}, ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
            return docs
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}: {str(e)}")
            return []
    
    def _execute_fallback_analysis(self, df: pd.DataFrame, query: str) -> Dict[str, Any]:
        """LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œì˜ fallback ë¶„ì„"""
        start_time = time.time()
        
        try:
            logger.info("Fallback ë¶„ì„ ëª¨ë“œ ì‹¤í–‰ - í†µê³„ì  ë¶„ì„ ìˆ˜í–‰")
            
            # ê¸°ë³¸ í†µê³„ ì •ë³´ ê³„ì‚°
            df_info = self._extract_data_info(df)
            analysis_prompt = self._generate_insurance_prompt(query, df_info)
            
            # ë°ì´í„° í†µê³„ ë¶„ì„
            stats = {}
            try:
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0:
                    stats = df.describe().to_dict()
                    logger.info("ê¸°ìˆ  í†µê³„ ê³„ì‚° ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"ê¸°ìˆ  í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            
            # ë³´í—˜ì‚¬ë³„ ìš”ì•½ (ê°€ëŠ¥í•œ ê²½ìš°)
            company_summary = {}
            for col in df.columns:
                if any(keyword in col.lower() for keyword in ['ë³´í—˜ì‚¬', 'company', 'insurer']):
                    try:
                        company_summary[col] = df[col].value_counts().to_dict()
                    except:
                        pass
            
            # ë³´í—˜ë£Œ ê´€ë ¨ ë¶„ì„ (ê°€ëŠ¥í•œ ê²½ìš°)
            premium_analysis = {}
            try:
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                for col in numeric_cols:
                    if any(keyword in col.lower() for keyword in ['ë³´í—˜ë£Œ', 'premium', 'ê¸ˆì•¡', 'amount']):
                        try:
                            premium_analysis[col] = {
                                'mean': float(df[col].mean()),
                                'min': float(df[col].min()),
                                'max': float(df[col].max()),
                                'std': float(df[col].std())
                            }
                        except:
                            pass
            except:
                pass
            
            # ë¶„ì„ ê²°ê³¼ êµ¬ì„±
            analysis_result = f"""
ë°ì´í„° í†µê³„ ë¶„ì„ ê²°ê³¼:

## ê¸°ë³¸ ì •ë³´
- ë°ì´í„° í˜•íƒœ: {df.shape}
- ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}

## ìˆ˜ì¹˜í˜• ë°ì´í„° ìš”ì•½
{df.describe().to_string() if df.select_dtypes(include=[np.number]).shape[1] > 0 else 'ìˆ˜ì¹˜í˜• ë°ì´í„° ì—†ìŒ'}

## ë³´í—˜ì‚¬ë³„ í˜„í™©
{chr(10).join([f'- {k}: {v}' for k, v in company_summary.items()]) if company_summary else 'ë³´í—˜ì‚¬ ì •ë³´ ì—†ìŒ'}

## ë³´í—˜ë£Œ ê´€ë ¨ í†µê³„
{chr(10).join([f'- {k}: í‰ê·  {v["mean"]:,.0f}, ìµœì†Œ {v["min"]:,.0f}, ìµœëŒ€ {v["max"]:,.0f}' for k, v in premium_analysis.items()]) if premium_analysis else 'ë³´í—˜ë£Œ ì •ë³´ ì—†ìŒ'}

## ë¶„ì„ ì œì•ˆ
ê³ ê° ì§ˆë¬¸ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì€ ì¶”ê°€ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:
1. íŠ¹ì • ë³´í—˜ì‚¬ë³„ ìƒì„¸ ë¹„êµ
2. ë³´í—˜ë£Œ ìˆ˜ì¤€ë³„ ìˆœìœ„ ë¶„ì„
3. ë³´ì¥ í•­ëª©ë³„ ì°¨ì´ì  ë¶„ì„
"""
            
            duration = time.time() - start_time
            logger.info(f"Fallback ë¶„ì„ ì™„ë£Œ - ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
            
            return {
                "status": "success",
                "analysis": analysis_result,
                "steps": [("fallback_analysis", "í†µê³„ì  ë¶„ì„ ìˆ˜í–‰")],
                "duration": duration,
                "mode": "fallback"
            }
            
        except Exception as e:
            logger.error(f"Fallback ë¶„ì„ ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
            return {"status": "error", "message": str(e)}
    
    def _execute_agent_analysis(self, df: pd.DataFrame, query: str) -> Dict[str, Any]:
        """LangChain agentë¥¼ í†µí•œ ë°ì´í„° ë¶„ì„ ì‹¤í–‰"""
        start_time = time.time()
        
        # LangChain ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ì²´í¬
        if not LANGCHAIN_AVAILABLE:
            logger.info("LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ Fallback ëª¨ë“œë¡œ ì „í™˜")
            return self._execute_fallback_analysis(df, query)
        
        try:
            # Agent ìƒì„±
            logger.info("LangChain Pandas Agent ìƒì„± ì‹œì‘")
            agent = self._create_pandas_agent(df)
            
            if agent is None:
                logger.warning("Agent ìƒì„± ì‹¤íŒ¨ - Fallback ëª¨ë“œë¡œ ì „í™˜")
                return self._execute_fallback_analysis(df, query)
            
            # ë³´í—˜ íŠ¹í™” í”„ë¡¬í”„íŠ¸
            df_info = self._extract_data_info(df)
            analysis_prompt = self._generate_insurance_prompt(query, df_info)
            
            logger.info(f"Agent ë¶„ì„ ì‹¤í–‰ ì‹œì‘ - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(analysis_prompt)}")
            
            # Agent ì‹¤í–‰
            result = agent.invoke({"input": analysis_prompt})
            
            # ê²°ê³¼ ì¶”ì¶œ
            if isinstance(result, dict) and 'output' in result:
                agent_result = result['output']
                steps = result.get('intermediate_steps', [])
                logger.info(f"Agent ë¶„ì„ ì™„ë£Œ - ê²°ê³¼ ê¸¸ì´: {len(agent_result)}, ë‹¨ê³„ ìˆ˜: {len(steps)}")
                
                # ë¶„ì„ ë‹¨ê³„ ë¡œê¹…
                for i, step in enumerate(steps):
                    logger.debug(f"Agent ë‹¨ê³„ {i+1}: {step}")
                
                return {
                    "status": "success", 
                    "analysis": agent_result,
                    "steps": steps,
                    "duration": time.time() - start_time,
                    "mode": "langchain_agent"
                }
            else:
                logger.error("Agent ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜ - Fallback ëª¨ë“œë¡œ ì „í™˜")
                return self._execute_fallback_analysis(df, query)
                
        except Exception as e:
            logger.error(f"Agent ì‹¤í–‰ ì˜¤ë¥˜: {type(e).__name__}: {str(e)} - Fallback ëª¨ë“œë¡œ ì „í™˜")
            return self._execute_fallback_analysis(df, query)
    
    def _generate_final_analysis(self, agent_result: Dict, query: str, df: pd.DataFrame) -> str:
        """ìµœì¢… LLMì„ í†µí•œ ì¢…í•© ë¶„ì„"""
        start_time = time.time()
        
        try:
            if agent_result["status"] != "success":
                return f"ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {agent_result['message']}"
            
            # ìµœì¢… ì¢…í•© í”„ë¡¬í”„íŠ¸
            final_prompt = f"""
ë‹¤ìŒì€ LangChain pandas agentì˜ ë³´í—˜ ë°ì´í„° ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

{agent_result['analysis']}

ê³ ê° ì§ˆë¬¸: {query}

ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë³´í—˜ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ì¢…í•©ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. **í•µì‹¬ ë¶„ì„ ë‚´ìš© ìš”ì•½**: ê°€ì¥ ì¤‘ìš”í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½
2. **ë³´í—˜ì‚¬ë³„ íŠ¹ì§• ë¹„êµ**: ê° ë³´í—˜ì‚¬ì˜ ì¥ì , ë‹¨ì , ì°¨ì´ì  ëª…í™•íˆ ë¹„êµ  
3. **ìˆ˜ì¹˜ ê¸°ë°˜ ì¶”ì²œ**: êµ¬ì²´ì ì¸ ê¸ˆì•¡ê³¼ ë°ì´í„°ë¥¼ ê·¼ê±°ë¡œ ì¶”ì²œ
4. **ì‹¤ì§ˆì ì¸ ì¡°ì–¸**: ê³ ê°ì˜ ì…ì¥ì—ì„œ ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ì´ ë  ì •ë³´ ì œê³µ

ë‹µë³€ í˜•ì‹:
- ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ ì‚¬ìš©
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ë°ì´í„° í¬í•¨  
- ë¶ˆë › í¬ì¸íŠ¸ë‚˜ ë²ˆí˜¸ë¡œ êµ¬ì¡°í™”
- ì „ë¬¸ê°€ì ì´ë©´ì„œ ì¹œì ˆí•œ í†¤
"""
            
            logger.info(f"ìµœì¢… ë¶„ì„ ìƒì„± ì‹œì‘ - LLM í˜¸ì¶œ")
            
            # ìµœì¢… LLM ì‘ë‹µ ìƒì„±
            response = self.llm(
                model="gemini-3-pro-preview", 
                contents=[final_prompt]
            )
            
            result_text = "ë¶„ì„ ê²°ê³¼ ìƒì„± ì‹¤íŒ¨"
            if response and hasattr(response, 'text') and response.text:
                result_text = response.text
            
            return result_text
            
            duration = time.time() - start_time
            logger.info(f"ìµœì¢… ë¶„ì„ ì™„ë£Œ - ê²°ê³¼ ê¸¸ì´: {len(result)}, ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
            
            return result
            
        except Exception as e:
            logger.error(f"ìµœì¢… ë¶„ì„ ìƒì„± ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
            return f"ìµœì¢… ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def _optimize_dataframe_memory(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„°í”„ë ˆì„ ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            original_memory = df.memory_usage(deep=True).sum()
            
            # ìˆ˜ì¹˜í˜• ë°ì´í„° ìµœì í™”
            for col in df.select_dtypes(include=['int64']).columns:
                df[col] = pd.to_numeric(df[col], downcast='integer')
            
            for col in df.select_dtypes(include=['float64']).columns:
                df[col] = pd.to_numeric(df[col], downcast='float')
            
            # ë¬¸ìì—´ ë°ì´í„° ìµœì í™”
            for col in df.select_dtypes(include=['object']).columns:
                if df[col].nunique() / len(df) < 0.5:  # ì¹´ë””ë„ë¦¬í‹°ê°€ ë‚®ì€ ê²½ìš°
                    df[col] = df[col].astype('category')
            
            optimized_memory = df.memory_usage(deep=True).sum()
            memory_reduction = (original_memory - optimized_memory) / original_memory * 100
            
            logger.info(f"ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ - {original_memory/1024/1024:.2f}MB â†’ {optimized_memory/1024/1024:.2f}MB ({memory_reduction:.1f}% ê°ì†Œ)")
            
            return df
            
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            return df
    
    def pandas_analysis(self, df: pd.DataFrame, query: str, comparison_table: Optional[pd.DataFrame] = None) -> str:
        """
        ê°œì„ ëœ ë³´í—˜ë£Œ ë°ì´í„° ë¶„ì„ - LangChain pandas agent í†µí•©
        2ë‹¨ê³„ êµ¬ì¡°: 1) Pandas Agent ë¶„ì„ â†’ 2) ìµœì¢… LLM ì¢…í•©
        """
        start_time = time.time()
        logger.info(f"=== 2ë‹¨ê³„ Pandas ë¶„ì„ ì‹œì‘ ===")
        logger.info(f"ì¿¼ë¦¬: '{query}', DataFrame í˜•íƒœ: {df.shape if df is not None else 'None'}")
        
        try:
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            if df is None or df.empty:
                logger.warning("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            # 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„
            prep_start = time.time()
            logger.info("1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ì‹œì‘")
            
            if comparison_table is not None and not comparison_table.empty:
                analysis_df = comparison_table
                data_type = "ë³´í—˜ì‚¬ë³„ ë¹„êµ í‘œ"
                logger.info(f"ë¹„êµ í‘œ ì‚¬ìš© - í˜•íƒœ: {comparison_table.shape}")
            else:
                # ë¹„êµ í‘œê°€ ì—†ëŠ” ê²½ìš° ì›ë³¸ ë°ì´í„°ë¡œ ë¹„êµ í‘œ ìƒì„± ì‹œë„
                logger.info("ë™ì  ë¹„êµ í‘œ ìƒì„± ì‹œë„")
                try:
                    from data_manager import data_manager
                    
                    if data_manager.coverage_premiums_df is not None and not data_manager.coverage_premiums_df.empty:
                        # ë™ì ìœ¼ë¡œ ë¹„êµ í‘œ ìƒì„±
                        logger.info("ë³´í—˜ë£Œ ë°ì´í„° ì •ê·œí™” ì‹œì‘")
                        normalized_df = data_manager.normalize_coverage_amounts(data_manager.coverage_premiums_df)
                        
                        logger.info("ë³´í—˜ì‚¬ë³„ ë°ì´í„° ì§‘ê³„ ì‹œì‘")
                        aggregated_df = data_manager.aggregate_coverage_by_code(normalized_df)
                        
                        logger.info("ë¹„êµ í‘œ ìƒì„± ì‹œì‘")
                        analysis_df = data_manager.create_comparison_table(aggregated_df)
                        data_type = "ë™ì  ìƒì„± ë¹„êµ í‘œ"
                        logger.info(f"ë™ì  ë¹„êµ í‘œ ìƒì„± ì™„ë£Œ - í˜•íƒœ: {analysis_df.shape}")
                    else:
                        analysis_df = df
                        data_type = "ì›ë³¸ ë°ì´í„°"
                        logger.info("ì›ë³¸ ë°ì´í„°ë¡œ ë¶„ì„ ì§„í–‰")
                except Exception as e:
                    logger.error(f"ë™ì  ë¹„êµ í‘œ ìƒì„± ì‹¤íŒ¨: {e}")
                    analysis_df = df
                    data_type = "ì›ë³¸ ë°ì´í„°(ë¹„êµ í‘œ ìƒì„± ì‹¤íŒ¨)"
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            analysis_df = self._optimize_dataframe_memory(analysis_df)
            
            prep_time = time.time() - prep_start
            logger.info(f"1ë‹¨ê³„ ì™„ë£Œ: ë°ì´í„° ì¤€ë¹„ - ìœ í˜•: {data_type}, ì†Œìš” ì‹œê°„: {prep_time:.2f}ì´ˆ")
            
            # 2ë‹¨ê³„: LangChain Pandas Agent ë¶„ì„
            logger.info("2ë‹¨ê³„: Pandas Agent ë¶„ì„ ì‹œì‘")
            agent_start = time.time()
            
            agent_result = self._execute_agent_analysis(analysis_df, query)
            
            agent_time = time.time() - agent_start
            logger.info(f"2ë‹¨ê³„ ì™„ë£Œ: Agent ë¶„ì„ - ìƒíƒœ: {agent_result['status']}, ì†Œìš” ì‹œê°„: {agent_time:.2f}ì´ˆ")
            
            # 3ë‹¨ê³„: ìµœì¢… LLM ì¢…í•© ë¶„ì„
            logger.info("3ë‹¨ê³„: ìµœì¢… LLM ì¢…í•© ë¶„ì„ ì‹œì‘")
            final_start = time.time()
            
            final_result = self._generate_final_analysis(agent_result, query, analysis_df)
            
            final_time = time.time() - final_start
            total_time = time.time() - start_time
            
            logger.info(f"3ë‹¨ê³„ ì™„ë£Œ: ìµœì¢… ë¶„ì„ - ì†Œìš” ì‹œê°„: {final_time:.2f}ì´ˆ")
            logger.info(f"=== ì „ì²´ ë¶„ì„ ì™„ë£Œ === ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ, ê²°ê³¼ ê¸¸ì´: {len(final_result)}ì")
            
            return final_result
            
        except Exception as e:
            error_time = time.time() - start_time
            logger.error(f"pandas_analysis ì¹˜ëª…ì  ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ ì‹œì : {error_time:.2f}ì´ˆ")
            logger.error(f"ë°ì´í„° ì •ë³´: shape={df.shape if df is not None else 'None'}")
            
            # ì‚¬ìš©ì ì¹œí™”ì  ì—ëŸ¬ ë©”ì‹œì§€
            error_message = self._generate_user_friendly_error(e)
            return error_message
    
    def _generate_user_friendly_error(self, error: Exception) -> str:
        """ì‚¬ìš©ì ì¹œí™”ì  ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±"""
        error_str = str(error).lower()
        error_type = type(error).__name__
        
        if "timeout" in error_str or "ì‹œê°„" in error_str:
            return "â° ë¶„ì„ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°ê°€ ë„ˆë¬´ ë§ê±°ë‚˜ ë³µì¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "memory" in error_str or "ë©”ëª¨ë¦¬" in error_str:
            return "ğŸ’¾ ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì•„ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¼ë¶€ ë°ì´í„°ë§Œ ë‹¤ì‹œ ë¶„ì„í•´ì£¼ì„¸ìš”."
        elif "api" in error_str or "ì—°ê²°" in error_str:
            return "ğŸ”Œ ì™¸ë¶€ API ì—°ê²°ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "parse" in error_str or "í˜•ì‹" in error_str:
            return "ğŸ“‹ ë°ì´í„° í˜•ì‹ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        else:
            return f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(error)} (ì˜¤ë¥˜ íƒ€ì…: {error_type})"
    
    def hybrid_chat(self, query: str, df: pd.DataFrame, insurance_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Hybrid RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ ì¢…í•©ì ì¸ ì§ˆì˜ì‘ë‹µ - ë¹„êµ í‘œ í™œìš©
        """
        start_time = time.time()
        logger.info(f"Hybrid RAG ì±— ì‹œì‘ - ì¿¼ë¦¬: '{query}'")
        logger.info(f"ì…ë ¥ ë°ì´í„° - DataFrame í˜•íƒœ: {df.shape if df is not None else 'None'}, ë³´í—˜ ë°ì´í„° ìˆ˜: {len(insurance_data) if insurance_data else 0}")
        
        try:
            # 1. ë²¡í„° ê²€ìƒ‰ì„ í†µí•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            logger.info("1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ ì‹œì‘")
            search_start = time.time()
            relevant_docs = self.search_relevant_docs(query)
            search_time = time.time() - search_start
            logger.info(f"1ë‹¨ê³„ ì™„ë£Œ: ë²¡í„° ê²€ìƒ‰ - ì°¾ì€ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}, ì†Œìš” ì‹œê°„: {search_time:.2f}ì´ˆ")
            
            # 2. ë¹„êµ í‘œ ìƒì„± ë° Pandas ë°ì´í„° ë¶„ì„
            pandas_result = ""
            comparison_table = None
            
            if df is not None and not df.empty:
                # ë¹„êµ í‘œ ìƒì„± ì‹œë„
                from data_manager import data_manager
                
                try:
                    if data_manager.coverage_premiums_df is not None and not data_manager.coverage_premiums_df.empty:
                        # ë™ì ìœ¼ë¡œ ë¹„êµ í‘œ ìƒì„±
                        normalized_df = data_manager.normalize_coverage_amounts(data_manager.coverage_premiums_df)
                        aggregated_df = data_manager.aggregate_coverage_by_code(normalized_df)
                        comparison_table = data_manager.create_comparison_table(aggregated_df)
                        
                        # ë¹„êµ í‘œë¥¼ ì‚¬ìš©í•œ ë¶„ì„
                        pandas_result = self.pandas_analysis(df, query, comparison_table)
                    else:
                        pandas_result = self.pandas_analysis(df, query)
                except Exception as e:
                    logger.warning(f"Failed to create comparison table for analysis: {e}")
                    pandas_result = self.pandas_analysis(df, query)
            
            # 3. ì¢…í•© ì‘ë‹µ ìƒì„±
            if relevant_docs and pandas_result:
                # ë‘ ê°€ì§€ ê²°ê³¼ ëª¨ë‘ ìˆëŠ” ê²½ìš°
                # Simple document-based QA using LLM directly
                context = "\n".join([doc['page_content'] for doc in relevant_docs])
                prompt = f"""Based on the following context, please answer the question: {query}

Context:
{context}

Answer:"""
                qa_result = self.llm(
                    model="gemini-3-pro-preview",
                    contents=[prompt]
                ).text
                combined_response = f"""ğŸ“Š **ë°ì´í„° ë¶„ì„ ê²°ê³¼:**\n{pandas_result}\n\nğŸ“‹ **ë³´ì¥ë‚´ìš© ê²€ìƒ‰ ê²°ê³¼:**\n{qa_result}"""
                
            elif relevant_docs:
                # ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ë§Œ ìˆëŠ” ê²½ìš°
                context = "\n".join([doc['page_content'] for doc in relevant_docs])
                prompt = f"""Based on the following context, please answer the question: {query}

Context:
{context}

Answer:"""
                qa_result = self.llm(
                    model="gemini-3-pro-preview",
                    contents=[prompt]
                ).text
                combined_response = f"""ğŸ“‹ **ë³´ì¥ë‚´ìš© ê²€ìƒ‰ ê²°ê³¼:**\n{qa_result}"""
                
            elif pandas_result:
                # Pandas ë¶„ì„ ê²°ê³¼ë§Œ ìˆëŠ” ê²½ìš°
                combined_response = f"""ğŸ“Š **ë°ì´í„° ë¶„ì„ ê²°ê³¼:**\n{pandas_result}"""
                
            else:
                combined_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            return {
                "response": combined_response,
                "sources_found": len(relevant_docs) > 0,
                "data_analysis_available": df is not None and not df.empty,
                "source_count": len(relevant_docs)
            }
            
        except Exception as e:
            logger.error(f"Error in hybrid chat: {e}")
            return {
                "response": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources_found": False,
                "data_analysis_available": False,
                "source_count": 0
            }

# ì „ì—­ Hybrid RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
rag_system = HybridRAGSystem()